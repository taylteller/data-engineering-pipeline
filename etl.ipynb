{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "The purpose of this project is to create an ETL pipeline that wrangles data on immigration, demographics and environmental factors (specifically temperature), in order to be able to glean insights about what factors may make certain US destination popular for international visits. Example questions that business users may want answered include:\n",
    "* Is there a correlation between a visitor's home temperature and the temperature of their chosen destination?\n",
    "* Do people from certain countries prefer destination cities where certain ethnicities are better represented?\n",
    "* Are particular destinations more popular with holders of visas of a certain type?\n",
    "\n",
    "We'll be using Spark to process the data.\n",
    "\n",
    "The project is broken down into 5 steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Clean Up the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's get ready by importing the libraries and tools we'll be using...\n",
    "\n",
    "import pandas as pd\n",
    "import helpers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, upper, first, dayofmonth, weekofyear, month, year, dayofweek, udf, monotonically_increasing_id, datediff\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ...and initializing our SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scoping the Project and Gathering Data\n",
    "\n",
    "### Scope \n",
    "We have various data on immigration, travel, temperature and demographics, from various sources. The business need is to be able to query the data to see what factors may affect immigration or US travel destination choices. More specifically, the goal is to create a set of relational tables against which we can run queries to look for patterns between chosen immigration or travel destination, and demographic and environmental factors.\n",
    "\n",
    "To accomplish this, Spark will be used, as it is well suited to process large amounts of data across multiple servers. If we wanted to take the project further, we could migrate the output data into a suitable RDBMS, like Redshift, and we could automate the pipleline with Airflow; this, however, is outside the scope of this particular project. The primary goal and scope of the task here is to explore the data, model it and load it into relational tables suitable for analytic querying.\n",
    "\n",
    "\n",
    "### Describing and Gathering the Data \n",
    "The source data are the following:\n",
    "\n",
    "* _I94 Immigration Data_: This data comes from the [US National Tourism and Trade Office](https://www.trade.gov/i-94-arrivals-program). It consists of the 2016 I-94 visitor arrivals data, providing information on arrivals for US visitors who stay 1 night or more. The link to the original dataset is defunct, but a copy of the data was captured and provided by Udacity.\n",
    "* _World Temperature Data_: [This dataset](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) was sourced from Kaggle. It consists of a table of global land temperatures by city and by month, from approximately 1750.\n",
    "* _U.S. City Demographic Data_: [This data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) comes from OpenSoft. The data is compiled from the US Census Bureau's 2015 American Community Survey. It contains demographic information for all US cities that have a population of 65,000 or more. \n",
    "* _Airport Code Table_: [This](https://datahub.io/core/airport-codes#data) is a table of airport codes and corresponding cities.\n",
    "\n",
    "To create the relational tables that will allows us to explore demographic and environmental correlations, we will only need data from the first three sets. We will not be using the _Airport Code Table_ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Exploring and Cleaning Up the Data\n",
    "Let's identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Some of this data was explored \"offline\", so to speak; information was gathered by visiting the original sources for the datasets or via supplemental contextual information. The i94 dataset, particularly, benefited from this; the definitions of the columns was understood primarily through looking at the `I94_SAS_Labels_Descriptions.SAS` file located in the same directory as this README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exploration\n",
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the i94 data\n",
    "i94_df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(i94_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the demographic data\n",
    "demog_df = spark.read.csv('us-cities-demographics.csv', inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(demog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the temperature data\n",
    "temp_df = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column, but first we cast the datetime column to string\n",
    "temp_df_string = temp_df.withColumn(\"dt\",col(\"dt\").cast(StringType()))\n",
    "helpers.show_total_missing_values(temp_df_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "#### Immigration Data\n",
    "If we look again at the table of total missing values per column in the _Exploration_ section, we can see that three columns have a total of more than 85% missing values: `occup`, `entdepu`, and `insnum`. These can be dropped.\n",
    "\n",
    "Taking this a step further, as we'll see shortly in our data model, there are only 11 columns we're interested in keeping. Let's select these to make the dataframe easier to work with and more legible to display. While we're at it, let's rename those columns and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.select(col('cicid').alias('id'),\n",
    "                             col('i94cit').alias('citizenship_country'),\n",
    "                             col('i94res').alias('residence_country'),\n",
    "                             col('i94port').alias('port_of_entry'),\n",
    "                             col('i94addr').alias('destination_state'),\n",
    "                             col('arrdate').alias('arrival_date'),\n",
    "                             col('depdate').alias('departure_date'),\n",
    "                             col('i94bir').alias('age'),\n",
    "                             col('i94visa').alias('visa_type'),\n",
    "                             col('dtaddto').alias('admitted_until'),\n",
    "                             col('gender').alias('gender')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We'll also convert the SAS dates in `arrdate` and `depdate`, and the string date in `dtaddto`, to PySpark dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.withColumn('arrival_date', helpers.convert_sas_date_to_datetime(i94_df.arrival_date)).cache()\n",
    "i94_df = i94_df.withColumn('departure_date', helpers.convert_sas_date_to_datetime(i94_df.departure_date)).cache()\n",
    "i94_df = i94_df.withColumn('admitted_until', to_date(col('admitted_until'),'MMddyyyy')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see that the fields `citizenship_country`, `residence_country`, and `state_of_arrival` are referenced by codes. These need to be cross-referenced with a list of codes that is currently only available in .SAS file. We'll update these values with their full names.\n",
    "\n",
    "We also have the `port_of_entry` field in the i94 table, which we can decompose into port city, port state, and port country. Doing this will later on allow us to add the port code to the temperature and demographic tables, which will allow us to keep our fact table simple, as a single field (`port_of_entry`) will be able to link three dimension tables together. \n",
    "\n",
    "All of this will serve to:\n",
    "1. make the values in our tables more meaningful, and\n",
    "2. make it easier to join tables when querying by reducing the number of joins and tables involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read our i94 fields and values legend \n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    i94_desc = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get country names by code\n",
    "country_codes = {}\n",
    "for countries in i94_desc[10:298]:\n",
    "    pair = countries.split('=')\n",
    "    code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    country_codes[code] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "origin_countries_df = spark.createDataFrame(list(country_codes.items()), ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "origin_countries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get city names and state abbreviations by numerical codes\n",
    "port_cities = {}\n",
    "port_states = {}\n",
    "broken_fields = {}\n",
    "for cities in i94_desc[303:893]:   \n",
    "    pair = cities.split('=')\n",
    "    code, location = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip()\n",
    "    city_and_state = location.split(',')\n",
    "    if len(city_and_state) == 2:\n",
    "        city, state = city_and_state[0].strip().strip(\"'\"), city_and_state[1].strip().strip(\"'\").strip()\n",
    "        port_cities[code] = city\n",
    "        port_states[code] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df = spark.createDataFrame(list(port_cities.items()), ['code', 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get state names by state abbreviation\n",
    "state_abbrs = {}\n",
    "for states in i94_desc[982:1035]:   \n",
    "    pair = states.split('=')\n",
    "    abbr, state = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip().strip(\"'\")\n",
    "    if 'N.' in state:\n",
    "        state = state.replace('N.','NORTH')\n",
    "    if 'S.' in state:\n",
    "        state = state.replace('S.','SOUTH')\n",
    "    if 'W.' in state:\n",
    "        state = state.replace('W.','WEST')\n",
    "    if 'DIST.' in state:\n",
    "        state = state.replace('DIST.','DISTRICT')\n",
    "    state_abbrs[abbr] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_countries = {}\n",
    "for code in port_states: \n",
    "    if port_states[code] in state_abbrs:\n",
    "        port_states[code] = state_abbrs[port_states[code]]\n",
    "        port_countries[code] = 'UNITED STATES'\n",
    "    else:\n",
    "        port_countries[code] = port_states[code]\n",
    "        port_states[code] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_abbrs_df = spark.createDataFrame(list(state_abbrs.items()), ['abbr','state'])\n",
    "port_states_df = spark.createDataFrame(list(port_states.items()), ['code','state'])\n",
    "port_countries_df = spark.createDataFrame(list(port_countries.items()), ['code','country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_abbrs_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_states_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_countries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create views from the dictionary dataframes we just created, \n",
    "# so we can use sql to add meaningful port city, state and country values to our i94 table\n",
    "i94_df.createOrReplaceTempView('i94_view')\n",
    "origin_countries_df.createOrReplaceTempView('origin_countries_view')\n",
    "state_abbrs_df.createOrReplaceTempView('state_abbrs_view')\n",
    "port_cities_df.createOrReplaceTempView('port_cities_view')\n",
    "port_states_df.createOrReplaceTempView('port_states_view')\n",
    "port_countries_df.createOrReplaceTempView('port_countries_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up our i94 table by converting codes to human-readable values\n",
    "i94_clean_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        i94.id AS immigration_id,\n",
    "        i94.age AS age,\n",
    "        i94.gender AS gender,\n",
    "        c_cit.country AS citizenship_country,\n",
    "        c_res.country AS residence_country,\n",
    "        i94.arrival_date AS arrival_date,\n",
    "        i94.departure_date AS departure_date,\n",
    "        i94.visa_type AS visa_type,\n",
    "        i94.admitted_until AS admitted_until,\n",
    "        i94.port_of_entry AS port_of_entry,\n",
    "        p_cit.city AS port_city,\n",
    "        ps.state AS port_state,\n",
    "        p_country.country AS port_country,\n",
    "        sa.state AS destination_state\n",
    "    FROM i94_view i94\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.citizenship_country\n",
    "    LEFT JOIN origin_countries_view c_cit\n",
    "    ON i94.citizenship_country = c_cit.code\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.residence_country\n",
    "    LEFT JOIN origin_countries_view c_res\n",
    "    ON i94.residence_country = c_res.code\n",
    "    \n",
    "    -- retrieve the city names of the code values in i94.port_of_entry\n",
    "    LEFT JOIN port_cities_view p_cit\n",
    "    ON i94.port_of_entry = p_cit.code\n",
    "    \n",
    "    -- retrieve the state names of the code values in i94.port_of_entry, if applicable\n",
    "    LEFT JOIN port_states_view ps\n",
    "    ON i94.port_of_entry = ps.code\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.port_of_entry\n",
    "    LEFT JOIN port_countries_view p_country\n",
    "    ON i94.port_of_entry = p_country.code\n",
    "    \n",
    "    -- retrieve the full state names of the state abbreviations in i94.destination_state\n",
    "    LEFT JOIN state_abbrs_view sa\n",
    "    ON i94.destination_state = sa.abbr\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_clean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the distribution of our destination_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('destination_state').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the distribution of port_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('port_city').count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data\n",
    "We'll start by renaming the columns and uppercasing the city and state names, to have consistent data formatting across our different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = demog_df.select(upper(col('City')).alias('city'),\n",
    "                             upper(col('State')).alias('state'),\n",
    "                             col('Median Age').alias('median_age'),\n",
    "                             col('Male Population').alias('male_population'),\n",
    "                             col('Female Population').alias('female_population'),\n",
    "                             col('Total Population').alias('total_population'),\n",
    "                             col('Number of Veterans').alias('total_veterans'),\n",
    "                             col('Foreign-born').alias('foreign_born'),\n",
    "                             col('Average Household Size').alias('average_household_size'),\n",
    "                             col('Race').alias('race'),\n",
    "                             col('Count').alias('race_count')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "While we may want denormalized data in our fact and dimension tables, we want to make sure that the denormalizations make sense for our purposes. If we look closely, we can observe that the `Race` column in the demographic data has a small number of unnecessarily repeating values. Barring a few anomalies, each cityappears as five separate rows with the same data for every field, except for the `Race` column, and the `Count`, which refers to the total number of residents who identify with the associated race in a given row. \n",
    "\n",
    "To simplify our table and our querying, we can simply pivot the `Race` column, meaning that we would take its five possible values and make each of them a single column. The values in the `Count` column would become the values for the associated `Race` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Turn the Race column into five distinct race columns\n",
    "demog_df = demog_df.groupBy(\"city\",\n",
    "                            \"state\",\n",
    "                            \"median_age\",\n",
    "                            \"male_population\",\n",
    "                            \"female_population\",\n",
    "                            \"total_population\",\n",
    "                            \"total_veterans\", \n",
    "                            \"foreign_born\", \n",
    "                            \"average_household_size\"\n",
    "                           ).pivot(\"race\").agg(first(\"race_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tidy the ethnicity column names\n",
    "demog_df = demog_df.select(col('city'),\n",
    "                           col('state'),\n",
    "                           col('median_age'),\n",
    "                           col('male_population'),\n",
    "                           col('female_population'),\n",
    "                           col('total_population'),\n",
    "                           col('total_veterans'),\n",
    "                           col('foreign_born'),\n",
    "                           col('average_household_size').alias('avg_household_size'),\n",
    "                           col('American Indian and Alaska Native').alias('indigenous'),\n",
    "                           col('Asian').alias('asian'),\n",
    "                           col('Black or African-American').alias('black'),\n",
    "                           col('Hispanic or Latino').alias('latinx'),\n",
    "                           col('White').alias('white'),\n",
    "                          ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data\n",
    "\n",
    "We saw in our data exploration that there were rows with missing AverageTemperature. These rows will be useless for our purposes, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df=temp_df.filter(temp_df.AverageTemperature != 'NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will also see shortly in our data model that the columns `AverageTemperatureUncertainty`, `Latitude` and `Longitude` are not relevant to our inquiry, so we will drop them and take the opportunity to rename our remaining columns, as well as to upprecase the city and country values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.select(col('dt').alias('date'),\n",
    "                             col('AverageTemperature').alias('avg_temperature'),\n",
    "                             upper(col('City')).alias('city'),\n",
    "                             upper(col('Country')).alias('country')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Additionally, our i94 dataset is specific to the year 2016. Our inquiry concerns itself with the factors that may have influenced visitors to choose the destinations they chose. The temperature dataset contains data from as far back as 1743. For our context, we will assume that people would not concern themselves with average temperatures more than 10 years old when choosing where to go in the short term. Hence, we will drop historical data from before 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop pre-2006 data, and make sure the date field doesn't get a midnight timestamp added (useless for our purposes)\n",
    "temp_df = temp_df.filter(temp_df.date >= '2006-01-01').withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "# temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "Data will be transferred into a star schema (one fact table where each dimension tables is referenced by a row; additional, derived information can also be added to the fact table). This schema design facilitates analytical querying, as it denormalizes data according to the particular aspects of the data we want to dig further into. In other words, it's optimized to minimize the number of JOINs required to query the data. \n",
    "\n",
    "#### Fact Table\n",
    "\n",
    "**visits_fact** - immigration events\n",
    "- visit_id, immigration_id, port_code, arrival_date, length_of_stay\n",
    "\n",
    "#### Dimension Tables\n",
    "\n",
    "**immigrations_dim** - i94 immigration details\n",
    "- immigration_id, age, gender, citizenship_country, residence_country, arrival_date, departure_date, visa_type, admitted_until, port_of_entry, port_city, port_state, port_country, destination_state\n",
    "\n",
    "**demographics_dim** - demographic data per major US city\n",
    "- city_id, port_code, city, state, median_age, male_population, female_population, total_population, total_veterans, foreign_born, avg_household_size, indigenous, asian, black, latinx, white\n",
    "\n",
    "**temperature_dim** - average temperatures per month from 2006-2013 for global cities\n",
    "- temperature_id, port_code, date, avg_temperature, city, country\n",
    "\n",
    "**date_dim** - dates of arrivals for visits\n",
    "- date, day, week, month, year, weekday\n",
    "\n",
    "![data model](./pipeline_star_schema.jpg \"data model\")\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "The steps necessary to pipeline the data into the above data model will be as follows:\n",
    "1. Clean the i94, temperature and demographics datasets (already performed)\n",
    "2. Create a date_dim dimension table based on the arrival_date values in the immigrations table, and extract the desired fields\n",
    "3. Create temperature_dim dimension table: \n",
    "    1. Load the existing, cleaned up temperature table, adding a unique id column\n",
    "    2. Add a port_code field based on the city value\n",
    "4. Create a demographic_dim dimension table:\n",
    "    1. Load the existing, cleaned up demographics table, adding a unique id column\n",
    "    2. Add a port_code field based on the city value\n",
    "5. Create the immigrations_dim table by loading the i94 table (we did a thorough job cleaning it up, knowing ahead of time what our data model was going to be; it already looks how we want it to)\n",
    "6. Create the visits_fact table with a derived length_of_stay column, mapping immigration_id from immigrations_dim, a port_code that can be referenced in temperature_dim and demographics_dim, and the arrival_date associated with the immigration_id\n",
    "7. Write these tables to parquet files that can be queried for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create date_dim table\n",
    "date_dim_df = i94_clean_df.withColumn(\"date\", col(\"arrival_date\"))\\\n",
    "                .withColumn(\"day\", dayofmonth(\"arrival_date\"))\\\n",
    "                .withColumn(\"week\", weekofyear(\"arrival_date\"))\\\n",
    "                .withColumn(\"month\", month(\"arrival_date\"))\\\n",
    "                .withColumn(\"year\", year(\"arrival_date\"))\\\n",
    "                .withColumn(\"weekday\", dayofweek(\"arrival_date\"))\n",
    "                        \n",
    "date_dim_df = date_dim_df.select(\"date\", \"day\", \"week\", \"month\", \"year\", \"weekday\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# UDF to add a port_code column to our temperature and demographics tables\n",
    "@udf()\n",
    "def get_city_port(city):\n",
    "    for key in port_cities:\n",
    "        if city in port_cities[key]:\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create temperature_dim table\n",
    "temperature_dim_df = temp_df.select(monotonically_increasing_id().alias(\"temperature_id\"),\n",
    "                                    col(\"date\"),\n",
    "                                    col(\"avg_temperature\"),\n",
    "                                    col(\"city\"),\n",
    "                                    col(\"country\"),\n",
    "                                   ).withColumn(\"port_code\", get_city_port(temp_df.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create demographics_dim table\n",
    "demographics_dim_df = demog_df.select(monotonically_increasing_id().alias(\"city_id\"),\n",
    "                                    col(\"city\"),\n",
    "                                    col(\"state\"),\n",
    "                                    col(\"median_age\"),\n",
    "                                    col(\"male_population\"),\n",
    "                                    col(\"female_population\"),\n",
    "                                    col(\"total_population\"),\n",
    "                                    col(\"total_veterans\"),\n",
    "                                    col(\"foreign_born\"),\n",
    "                                    col(\"avg_household_size\"),\n",
    "                                    col(\"indigenous\"),\n",
    "                                    col(\"asian\"),\n",
    "                                    col(\"black\"),\n",
    "                                    col(\"latinx\"),\n",
    "                                    col(\"white\"),\n",
    "                                   ).withColumn(\"port_code\", get_city_port(demog_df.city))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create immigrations_dim table\n",
    "immigrations_dim_df = i94_clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create visits_fact table\n",
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "visits_fact_df = immigrations_dim_df.select(monotonically_increasing_id().alias(\"visit_id\"),\n",
    "                                    col(\"immigration_id\"),\n",
    "                                    col(\"port_of_entry\").alias(\"port_code\"),\n",
    "                                    col(\"arrival_date\"),\n",
    "                                    datediff(col(\"departure_date\"),col(\"arrival_date\")).alias(\"length_of_stay\")\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create parquet files for analysis\n",
    "%rm -rf ./results/\n",
    "immigrations_dim_df.write.mode('append').partitionBy('age').parquet('./results/immigrations_dim.parquet')\n",
    "demographics_dim_df.write.mode('append').partitionBy('state').parquet('./results/demographics_dim.parquet')\n",
    "temperature_dim_df.write.mode('append').partitionBy('country').parquet('./results/temperature_dim.parquet')\n",
    "date_dim_df.write.mode('append').parquet('./results/date_dim.parquet')\n",
    "visits_fact_df.write.mode('append').partitionBy('arrival_date').parquet('./results/visits_fact.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
