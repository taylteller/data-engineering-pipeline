{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "The purpose of this project is to create an ETL pipeline that wrangles data on immigration, demographics and environmental factors (specifically temperature), in order to be able to glean insights about what factors may make certain US destination popular for international visits. Example questions that business users may want answered include:\n",
    "* Is there a correlation between a visitor's home temperature and the temperature of their chosen destination?\n",
    "* Do people from certain countries prefer destination cities where certain ethnicities are better represented?\n",
    "* Are particular destinations more popular with holders of visas of a certain type?\n",
    "\n",
    "We'll be using Spark to process the data.\n",
    "\n",
    "The project is broken down into 5 steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Clean Up the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's get ready by importing the libraries and tools we'll be using...\n",
    "\n",
    "import pandas as pd\n",
    "import helpers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, upper, first\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ...and initializing our SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scoping the Project and Gathering Data\n",
    "\n",
    "### Scope \n",
    "We have various data on immigration, travel, temperature and demographics, from various sources. The business need is to be able to query the data to see what factors may affect immigration or US travel destination choices. More specifically, the goal is to create a set of relational tables against which we can run queries to look for patterns between chosen immigration or travel destination, and demographic and environmental factors.\n",
    "\n",
    "To accomplish this, Spark will be used, as it is well suited to process large amounts of data across multiple servers. If we wanted to take the project further, we could migrate the output data into a suitable RDBMS, like Redshift, and we could automate the pipleline with Airflow; this, however, is outside the scope of this particular project. The primary goal and scope of the task here is to explore the data, model it and load it into relational tables suitable for analytic querying.\n",
    "\n",
    "\n",
    "### Describing and Gathering the Data \n",
    "The source data are the following:\n",
    "\n",
    "* _I94 Immigration Data_: This data comes from the [US National Tourism and Trade Office](https://www.trade.gov/i-94-arrivals-program). It consists of the 2016 I-94 visitor arrivals data, providing information on arrivals for US visitors who stay 1 night or more. The link to the original dataset is defunct, but a copy of the data was captured and provided by Udacity.\n",
    "* _World Temperature Data_: [This dataset](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) was sourced from Kaggle. It consists of a table of global land temperatures by city and by month, from approximately 1750.\n",
    "* _U.S. City Demographic Data_: [This data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) comes from OpenSoft. The data is compiled from the US Census Bureau's 2015 American Community Survey. It contains demographic information for all US cities that have a population of 65,000 or more. \n",
    "* _Airport Code Table_: [This](https://datahub.io/core/airport-codes#data) is a table of airport codes and corresponding cities.\n",
    "\n",
    "To create the relational tables that will allows us to explore demographic and environmental correlations, we will only need data from the first three sets. We will not be using the _Airport Code Table_ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Exploring and Cleaning Up the Data\n",
    "Let's identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Some of this data was explored \"offline\", so to speak; information was gathered by visiting the original sources for the datasets or via supplemental contextual information. The i94 dataset, particularly, benefited from this; the definitions of the columns was understood primarily through looking at the `I94_SAS_Labels_Descriptions.SAS` file located in the same directory as this README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exploration\n",
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the i94 data\n",
    "i94_df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(i94_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the demographic data\n",
    "demog_df = spark.read.csv('us-cities-demographics.csv', inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040</td>\n",
       "      <td>46799</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819</td>\n",
       "      <td>8229</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127</td>\n",
       "      <td>87105</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821</td>\n",
       "      <td>33878</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040</td>\n",
       "      <td>143873</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829</td>\n",
       "      <td>86253</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8            40601   \n",
       "1            Quincy  Massachusetts        41.0            44129   \n",
       "2            Hoover        Alabama        38.5            38040   \n",
       "3  Rancho Cucamonga     California        34.5            88127   \n",
       "4            Newark     New Jersey        34.6           138040   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0              41862             82463                1562         30908   \n",
       "1              49500             93629                4147         32935   \n",
       "2              46799             84839                4819          8229   \n",
       "3              87105            175232                5821         33878   \n",
       "4             143873            281913                5829         86253   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a preview\n",
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column name</th>\n",
       "      <th>total missing</th>\n",
       "      <th>% missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>State</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Median Age</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Male Population</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Female Population</td>\n",
       "      <td>3</td>\n",
       "      <td>0.103770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Total Population</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Number of Veterans</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Foreign-born</td>\n",
       "      <td>13</td>\n",
       "      <td>0.449671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Average Household Size</td>\n",
       "      <td>16</td>\n",
       "      <td>0.553442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>State Code</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Race</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               column name  total missing  % missing\n",
       "0                     City              0   0.000000\n",
       "1                    State              0   0.000000\n",
       "2               Median Age              0   0.000000\n",
       "3          Male Population              3   0.103770\n",
       "4        Female Population              3   0.103770\n",
       "5         Total Population              0   0.000000\n",
       "6       Number of Veterans             13   0.449671\n",
       "7             Foreign-born             13   0.449671\n",
       "8   Average Household Size             16   0.553442\n",
       "9               State Code              0   0.000000\n",
       "10                    Race              0   0.000000\n",
       "11                   Count              0   0.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(demog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the temperature data\n",
    "temp_df = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Århus   \n",
       "1 1743-12-01                 NaN                            NaN  Århus   \n",
       "2 1744-01-01                 NaN                            NaN  Århus   \n",
       "3 1744-02-01                 NaN                            NaN  Århus   \n",
       "4 1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a preview\n",
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column name</th>\n",
       "      <th>total missing</th>\n",
       "      <th>% missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AverageTemperature</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AverageTemperatureUncertainty</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     column name  total missing  % missing\n",
       "0                             dt              0        0.0\n",
       "1             AverageTemperature              0        0.0\n",
       "2  AverageTemperatureUncertainty              0        0.0\n",
       "3                           City              0        0.0\n",
       "4                        Country              0        0.0\n",
       "5                       Latitude              0        0.0\n",
       "6                      Longitude              0        0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the total of null or NaN values per column, but first we cast the datetime column to string\n",
    "temp_df_string = temp_df.withColumn(\"dt\",col(\"dt\").cast(StringType()))\n",
    "helpers.show_total_missing_values(temp_df_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "#### Immigration Data\n",
    "If we look again at the table of total missing values per column in the _Exploration_ section, we can see that three columns have a total of more than 85% missing values: `occup`, `entdepu`, and `insnum`. These can be dropped.\n",
    "\n",
    "Taking this a step further, as we'll see shortly in our data model, there are only 11 columns we're interested in keeping. Let's select these to make the dataframe easier to work with and more legible to display. While we're at it, let's rename those columns and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.select(col('cicid').alias('id'),\n",
    "                             col('i94cit').alias('citizenship_country'),\n",
    "                             col('i94res').alias('residence_country'),\n",
    "                             col('i94port').alias('port_of_entry'),\n",
    "                             col('i94addr').alias('destination_state'),\n",
    "                             col('arrdate').alias('arrival_date'),\n",
    "                             col('depdate').alias('departure_date'),\n",
    "                             col('i94bir').alias('age'),\n",
    "                             col('i94visa').alias('visa_type'),\n",
    "                             col('dtaddto').alias('admitted_until'),\n",
    "                             col('gender').alias('gender')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We'll also convert the SAS dates in `arrdate` and `depdate`, and the string date in `dtaddto`, to PySpark dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.withColumn('arrival_date', helpers.convert_sas_date_to_datetime(i94_df.arrival_date)).cache()\n",
    "i94_df = i94_df.withColumn('departure_date', helpers.convert_sas_date_to_datetime(i94_df.departure_date)).cache()\n",
    "i94_df = i94_df.withColumn('admitted_until', to_date(col('admitted_until'),'MMddyyyy')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see that the fields `citizenship_country`, `residence_country`, `port_of_entry`, and `state_of_arrival` are referenced by codes. These need to be cross-referenced with a list of codes that is currently only available in text form. Since temperature and demographic data have city, state and country fields written out in full, we can parse the text file of codes and update the values in the i94 dataframe to also be written out in full. This will make it easier to join tables when querying by reducing the number of joins and tables involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read our i94 fields and values legend \n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    i94_desc = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get country names by code\n",
    "country_codes = {}\n",
    "for countries in i94_desc[10:298]:\n",
    "    pair = countries.split('=')\n",
    "    code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    country_codes[code] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries_df = spark.createDataFrame(list(country_codes.items()), ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "| 324|     ANGOLA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get city names and state abbreviations by numerical codes\n",
    "port_cities = {}\n",
    "port_states = {}\n",
    "broken_fields = {}\n",
    "for cities in i94_desc[303:893]:   \n",
    "    pair = cities.split('=')\n",
    "    code, location = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip()\n",
    "    city_and_state = location.split(',')\n",
    "    if len(city_and_state) == 2:\n",
    "        city, state = city_and_state[0].strip().strip(\"'\"), city_and_state[1].strip().strip(\"'\").strip()\n",
    "        port_cities[code] = city\n",
    "        port_states[code] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df = spark.createDataFrame(list(port_cities.items()), ['code', 'city'])\n",
    "port_states_df = spark.createDataFrame(list(port_states.items()), ['code','state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_states_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get state names by state abbreviation\n",
    "state_codes = {}\n",
    "for states in i94_desc[982:1035]:   \n",
    "    pair = states.split('=')\n",
    "    code, state = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip().strip(\"'\")\n",
    "    if 'N.' in state:\n",
    "        state = state.replace('N.','NORTH')\n",
    "    if 'S.' in state:\n",
    "        state = state.replace('S.','SOUTH')\n",
    "    if 'W.' in state:\n",
    "        state = state.replace('W.','WEST')\n",
    "    if 'DIST.' in state:\n",
    "        state = state.replace('DIST.','DISTRICT')\n",
    "    state_codes[code] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_codes_df = spark.createDataFrame(list(state_codes.items()), ['code', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+----------------+----------+-----------------+\n",
      "|       id|arrival_date|departure_date| age|visa_type|admitted_until|gender|citizenship_country|residence_country|       port_city|port_state|destination_state|\n",
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+----------------+----------+-----------------+\n",
      "|4651287.0|  2016-04-24|    2016-04-29|45.0|      2.0|    2016-10-24|     F|               null|             null|NEWARK/TETERBORO|NEW JERSEY|             null|\n",
      "|2014736.0|  2016-04-11|    2016-04-15|38.0|      2.0|    2016-07-09|  null|               null|          GERMANY|         HOUSTON|     TEXAS|             null|\n",
      "|2093085.0|  2016-04-11|    2016-04-15|46.0|      1.0|    2016-10-11|     M|               null|             null|         HOUSTON|     TEXAS|             null|\n",
      "| 998502.0|  2016-04-06|    2016-04-10|32.0|      2.0|    2016-07-04|     M|              SPAIN|            SPAIN|         HOUSTON|     TEXAS|             null|\n",
      "|1594614.0|  2016-04-09|    2016-04-10|21.0|      1.0|    2016-10-08|  null|     UNITED KINGDOM|   UNITED KINGDOM|        NEW YORK|  NEW YORK|             null|\n",
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+----------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create views from the dictionary dataframes we just created, \n",
    "# so we can use sql to replace the codes in our i94 table with their more human-usable values\n",
    "i94_df.createOrReplaceTempView('i94_view')\n",
    "countries_df.createOrReplaceTempView('countries_view')\n",
    "port_cities_df.createOrReplaceTempView('port_cities_view')\n",
    "port_states_df.createOrReplaceTempView('port_states_view')\n",
    "state_codes_df.createOrReplaceTempView('state_codes_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up our i94 table by converting codes to human-readable values\n",
    "i94_clean_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        i94.id,\n",
    "        i94.arrival_date AS arrival_date,\n",
    "        i94.departure_date AS departure_date,\n",
    "        i94.age AS age,\n",
    "        i94.visa_type AS visa_type,\n",
    "        i94.admitted_until AS admitted_until,\n",
    "        i94.gender AS gender,\n",
    "        c_cit.country AS citizenship_country,\n",
    "        c_res.country AS residence_country,\n",
    "        pc.city AS port_city,\n",
    "        sc_port.state AS port_state,\n",
    "        sc.state AS destination_state\n",
    "    FROM i94_view i94\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.citizenship_country\n",
    "    LEFT JOIN countries_view c_cit\n",
    "    ON i94.citizenship_country = c_cit.code\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.residence_country\n",
    "    LEFT JOIN countries_view c_res\n",
    "    ON i94.residence_country = c_res.code\n",
    "    \n",
    "    -- retrieve the city names of the code values in i94.port_of_entry\n",
    "    LEFT JOIN port_cities_view pc\n",
    "    ON i94.port_of_entry = pc.code\n",
    "    \n",
    "    -- retrieve the state abbreviations of the numerical codes in i94.port_of_entry...\n",
    "    -- ...then the full state names of the state abbreviations\n",
    "    LEFT JOIN port_states_view ps\n",
    "    ON i94.port_of_entry = ps.code\n",
    "    LEFT JOIN state_codes_view sc_port\n",
    "    ON ps.state = sc_port.code \n",
    "    \n",
    "    -- retrieve the full state names of the state abbreviations in i94.destination_state\n",
    "    LEFT JOIN state_codes_view sc\n",
    "    ON i94.destination_state = sc.code\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_clean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|   destination_state| count|\n",
      "+--------------------+------+\n",
      "|          NEW JERSEY| 76531|\n",
      "|        PENNSYLVANIA| 30293|\n",
      "|            ILLINOIS| 82126|\n",
      "|DISTRICT OF COLUMBIA| 28228|\n",
      "|            MARYLAND| 25360|\n",
      "|       WEST VIRGINIA|   808|\n",
      "|               IDAHO|  1752|\n",
      "|            MISSOURI|  8484|\n",
      "|             MONTANA|  1339|\n",
      "|            MICHIGAN| 32101|\n",
      "|             FLORIDA|621701|\n",
      "|                null|187354|\n",
      "|              OREGON| 12574|\n",
      "|        SOUTH DAKOTA|   557|\n",
      "|           LOUISIANA| 22655|\n",
      "|              ALASKA|  1604|\n",
      "|         PUERTO RICO|  9474|\n",
      "|      VIRGIN ISLANDS|   226|\n",
      "|               MAINE|  2361|\n",
      "|       NEW HAMPSHIRE|  2817|\n",
      "|            OKLAHOMA|  3239|\n",
      "|            VIRGINIA| 31399|\n",
      "|          WASHINGTON| 55792|\n",
      "|      NORTH CAROLINA| 23375|\n",
      "|             WYOMING|   460|\n",
      "|               TEXAS|134321|\n",
      "|            NEBRASKA| 26574|\n",
      "|           MINNESOTA| 11194|\n",
      "|              HAWAII|168764|\n",
      "|                GUAM| 94107|\n",
      "|        RHODE ISLAND|  3289|\n",
      "|         MISSISSIPPI|  1771|\n",
      "|           TENNESSEE| 12105|\n",
      "|           WISCONSON|  7860|\n",
      "|            COLORADO| 15874|\n",
      "|              NEVADA|114609|\n",
      "|             VERMONT|  1477|\n",
      "|          NEW MEXICO|  1994|\n",
      "|            NEW YORK|553677|\n",
      "|                UTAH|  7551|\n",
      "|          CALIFORNIA|470386|\n",
      "|                IOWA|  3391|\n",
      "|              KANSAS|  3224|\n",
      "|             ARIZONA| 20218|\n",
      "|            KENTUCKY|  5790|\n",
      "|                OHIO| 18089|\n",
      "|       MASSACHUSETTS| 70486|\n",
      "|      SOUTH CAROLINA|  9811|\n",
      "|            DELAWARE|  3111|\n",
      "|         CONNECTICUT| 13991|\n",
      "|        NORTH DAKOTA|  1225|\n",
      "|            ARKANSAS|  2873|\n",
      "|             INDIANA| 11278|\n",
      "|             GEORGIA| 44663|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of our destination_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('destination_state').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the distribution of port_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('port_state').count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data\n",
    "We'll start by renaming the columns and uppercasing the city and state names, to have consistent data formatting across our different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = demog_df.select(upper(col('City')).alias('city'),\n",
    "                             upper(col('State')).alias('state'),\n",
    "                             col('Median Age').alias('median_age'),\n",
    "                             col('Male Population').alias('male_population'),\n",
    "                             col('Female Population').alias('female_population'),\n",
    "                             col('Total Population').alias('total_population'),\n",
    "                             col('Number of Veterans').alias('total_veterans'),\n",
    "                             col('Foreign-born').alias('foreign_born'),\n",
    "                             col('Average Household Size').alias('average_household_size'),\n",
    "                             col('Race').alias('race'),\n",
    "                             col('Count').alias('race_count')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+--------------------+----------+\n",
      "|       city|     state|median_age|male_population|female_population|total_population|total_veterans|foreign_born|average_household_size|                race|race_count|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+--------------------+----------+\n",
      "|TALLAHASSEE|   FLORIDA|      26.2|          89390|           100504|          189894|          9575|       16720|                  2.38|  Hispanic or Latino|     13538|\n",
      "|  VANCOUVER|WASHINGTON|      37.2|          82958|            89895|          172853|         12391|       21748|                  2.49|  Hispanic or Latino|     23184|\n",
      "|  MILWAUKEE| WISCONSIN|      31.6|         286315|           313839|          600154|         20615|       58321|                  2.51|American Indian a...|     10813|\n",
      "| RICHARDSON|     TEXAS|      35.5|          54676|            56151|          110827|          4797|       29579|                  2.83|               White|     76116|\n",
      "|   SURPRISE|   ARIZONA|      39.4|          62875|            65567|          128442|         11109|        9929|                   2.9|               White|    115419|\n",
      "+-----------+----------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demog_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "While we may want denormalized data in our fact and dimension tables, we want to make sure that the denormalizations make sense for our purposes. If we look closely, we can observe that the `Race` column in the demographic data has a small number of unnecessarily repeating values. The `Count` column refers to the total number of residents who identify with the associated race in a given row. To simplify our table and our querying, we can simply pivot the `Race` column, meaning that we would take its five possible values and make each of them a single column. The values in the `Count` column would become the values for the associated `Race` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|       city|         state|median_age|male_population|female_population|total_population|total_veterans|foreign_born|average_household_size|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+-----------+--------------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|   COLUMBIA|SOUTH CAROLINA|      28.0|          67686|            65707|          133393|          5708|        6074|                  2.32|                             1420| 3501|                    56398|              7545| 73232|\n",
      "|BLOOMINGTON|     MINNESOTA|      40.9|          43318|            43118|           86436|          6176|       10728|                   2.3|                             1745| 4689|                     5828|              8021| 71874|\n",
      "| UNION CITY|    NEW JERSEY|      35.4|          35376|            33773|           69149|           705|       40553|                  2.85|                              545| 4044|                     4686|             53174| 50031|\n",
      "|    GARLAND|         TEXAS|      34.5|         116406|           120430|          236836|         10407|       62975|                  3.12|                             3083|27217|                    40507|             90989|154484|\n",
      "| SAN ANGELO|         TEXAS|      32.8|          49669|            50744|          100413|          8412|        7859|                   2.5|                              548| 1658|                     6079|             42494| 86655|\n",
      "+-----------+--------------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn the Race column into five distinct race columns\n",
    "demog_df = demog_df.groupBy(\"city\",\n",
    "                            \"state\",\n",
    "                            \"median_age\",\n",
    "                            \"male_population\",\n",
    "                            \"female_population\",\n",
    "                            \"total_population\",\n",
    "                            \"total_veterans\", \n",
    "                            \"foreign_born\", \n",
    "                            \"average_household_size\"\n",
    "                           ).pivot(\"race\").agg(first(\"race_count\"))\n",
    "demog_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>total_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>indigenous</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>latinx</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>28.0</td>\n",
       "      <td>67686</td>\n",
       "      <td>65707</td>\n",
       "      <td>133393</td>\n",
       "      <td>5708</td>\n",
       "      <td>6074</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1420</td>\n",
       "      <td>3501</td>\n",
       "      <td>56398</td>\n",
       "      <td>7545</td>\n",
       "      <td>73232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLOOMINGTON</td>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>40.9</td>\n",
       "      <td>43318</td>\n",
       "      <td>43118</td>\n",
       "      <td>86436</td>\n",
       "      <td>6176</td>\n",
       "      <td>10728</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1745</td>\n",
       "      <td>4689</td>\n",
       "      <td>5828</td>\n",
       "      <td>8021</td>\n",
       "      <td>71874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNION CITY</td>\n",
       "      <td>NEW JERSEY</td>\n",
       "      <td>35.4</td>\n",
       "      <td>35376</td>\n",
       "      <td>33773</td>\n",
       "      <td>69149</td>\n",
       "      <td>705</td>\n",
       "      <td>40553</td>\n",
       "      <td>2.85</td>\n",
       "      <td>545</td>\n",
       "      <td>4044</td>\n",
       "      <td>4686</td>\n",
       "      <td>53174</td>\n",
       "      <td>50031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GARLAND</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>34.5</td>\n",
       "      <td>116406</td>\n",
       "      <td>120430</td>\n",
       "      <td>236836</td>\n",
       "      <td>10407</td>\n",
       "      <td>62975</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3083</td>\n",
       "      <td>27217</td>\n",
       "      <td>40507</td>\n",
       "      <td>90989</td>\n",
       "      <td>154484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAN ANGELO</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>32.8</td>\n",
       "      <td>49669</td>\n",
       "      <td>50744</td>\n",
       "      <td>100413</td>\n",
       "      <td>8412</td>\n",
       "      <td>7859</td>\n",
       "      <td>2.50</td>\n",
       "      <td>548</td>\n",
       "      <td>1658</td>\n",
       "      <td>6079</td>\n",
       "      <td>42494</td>\n",
       "      <td>86655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CITRUS HEIGHTS</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>37.8</td>\n",
       "      <td>41982</td>\n",
       "      <td>45071</td>\n",
       "      <td>87053</td>\n",
       "      <td>6171</td>\n",
       "      <td>9466</td>\n",
       "      <td>2.49</td>\n",
       "      <td>1514</td>\n",
       "      <td>3615</td>\n",
       "      <td>5979</td>\n",
       "      <td>14661</td>\n",
       "      <td>77525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GILBERT</td>\n",
       "      <td>ARIZONA</td>\n",
       "      <td>33.2</td>\n",
       "      <td>116711</td>\n",
       "      <td>130812</td>\n",
       "      <td>247523</td>\n",
       "      <td>10817</td>\n",
       "      <td>24531</td>\n",
       "      <td>3.20</td>\n",
       "      <td>5965</td>\n",
       "      <td>19740</td>\n",
       "      <td>9076</td>\n",
       "      <td>39937</td>\n",
       "      <td>211322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>APPLE VALLEY</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>34.3</td>\n",
       "      <td>32873</td>\n",
       "      <td>39312</td>\n",
       "      <td>72185</td>\n",
       "      <td>5714</td>\n",
       "      <td>5801</td>\n",
       "      <td>3.03</td>\n",
       "      <td>1446</td>\n",
       "      <td>2281</td>\n",
       "      <td>9124</td>\n",
       "      <td>25928</td>\n",
       "      <td>60767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DALY CITY</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>39.7</td>\n",
       "      <td>53817</td>\n",
       "      <td>52757</td>\n",
       "      <td>106574</td>\n",
       "      <td>3782</td>\n",
       "      <td>56640</td>\n",
       "      <td>3.26</td>\n",
       "      <td>893</td>\n",
       "      <td>61918</td>\n",
       "      <td>5401</td>\n",
       "      <td>24316</td>\n",
       "      <td>25522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CARLSBAD</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>42.1</td>\n",
       "      <td>55119</td>\n",
       "      <td>58347</td>\n",
       "      <td>113466</td>\n",
       "      <td>6031</td>\n",
       "      <td>17689</td>\n",
       "      <td>2.68</td>\n",
       "      <td>1513</td>\n",
       "      <td>11948</td>\n",
       "      <td>876</td>\n",
       "      <td>12969</td>\n",
       "      <td>98705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CLOVIS</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>37.8</td>\n",
       "      <td>52392</td>\n",
       "      <td>51780</td>\n",
       "      <td>104172</td>\n",
       "      <td>6173</td>\n",
       "      <td>13409</td>\n",
       "      <td>2.76</td>\n",
       "      <td>1876</td>\n",
       "      <td>14249</td>\n",
       "      <td>3434</td>\n",
       "      <td>23744</td>\n",
       "      <td>78029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SPOKANE</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>36.6</td>\n",
       "      <td>102756</td>\n",
       "      <td>110511</td>\n",
       "      <td>213267</td>\n",
       "      <td>18044</td>\n",
       "      <td>13253</td>\n",
       "      <td>2.34</td>\n",
       "      <td>7192</td>\n",
       "      <td>9059</td>\n",
       "      <td>10045</td>\n",
       "      <td>13365</td>\n",
       "      <td>193143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BAKERSFIELD</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>30.6</td>\n",
       "      <td>182154</td>\n",
       "      <td>191473</td>\n",
       "      <td>373627</td>\n",
       "      <td>12284</td>\n",
       "      <td>71575</td>\n",
       "      <td>3.23</td>\n",
       "      <td>7357</td>\n",
       "      <td>31929</td>\n",
       "      <td>34240</td>\n",
       "      <td>180241</td>\n",
       "      <td>272853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>KIRKLAND</td>\n",
       "      <td>WASHINGTON</td>\n",
       "      <td>40.8</td>\n",
       "      <td>42159</td>\n",
       "      <td>45108</td>\n",
       "      <td>87267</td>\n",
       "      <td>5156</td>\n",
       "      <td>18570</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1155</td>\n",
       "      <td>15277</td>\n",
       "      <td>1041</td>\n",
       "      <td>6720</td>\n",
       "      <td>70491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LAKEWOOD</td>\n",
       "      <td>COLORADO</td>\n",
       "      <td>37.7</td>\n",
       "      <td>76013</td>\n",
       "      <td>76576</td>\n",
       "      <td>152589</td>\n",
       "      <td>9988</td>\n",
       "      <td>14169</td>\n",
       "      <td>2.29</td>\n",
       "      <td>2597</td>\n",
       "      <td>7340</td>\n",
       "      <td>2679</td>\n",
       "      <td>33630</td>\n",
       "      <td>139669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>REDLANDS</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>36.2</td>\n",
       "      <td>33993</td>\n",
       "      <td>37035</td>\n",
       "      <td>71028</td>\n",
       "      <td>3445</td>\n",
       "      <td>8681</td>\n",
       "      <td>2.82</td>\n",
       "      <td>811</td>\n",
       "      <td>6255</td>\n",
       "      <td>6973</td>\n",
       "      <td>19698</td>\n",
       "      <td>55241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>EL CAJON</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>32.7</td>\n",
       "      <td>54450</td>\n",
       "      <td>49238</td>\n",
       "      <td>103688</td>\n",
       "      <td>7103</td>\n",
       "      <td>31865</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1891</td>\n",
       "      <td>4561</td>\n",
       "      <td>7534</td>\n",
       "      <td>31542</td>\n",
       "      <td>84703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>SANTA BARBARA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "      <td>37.8</td>\n",
       "      <td>45068</td>\n",
       "      <td>46784</td>\n",
       "      <td>91852</td>\n",
       "      <td>4518</td>\n",
       "      <td>19441</td>\n",
       "      <td>2.51</td>\n",
       "      <td>1560</td>\n",
       "      <td>4964</td>\n",
       "      <td>2013</td>\n",
       "      <td>31102</td>\n",
       "      <td>73659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>SYRACUSE</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>30.3</td>\n",
       "      <td>69462</td>\n",
       "      <td>74690</td>\n",
       "      <td>144152</td>\n",
       "      <td>5845</td>\n",
       "      <td>17733</td>\n",
       "      <td>2.39</td>\n",
       "      <td>3606</td>\n",
       "      <td>9386</td>\n",
       "      <td>46026</td>\n",
       "      <td>13368</td>\n",
       "      <td>88679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NASHVILLE</td>\n",
       "      <td>TENNESSEE</td>\n",
       "      <td>34.1</td>\n",
       "      <td>314231</td>\n",
       "      <td>340365</td>\n",
       "      <td>654596</td>\n",
       "      <td>27942</td>\n",
       "      <td>88193</td>\n",
       "      <td>2.39</td>\n",
       "      <td>5474</td>\n",
       "      <td>27355</td>\n",
       "      <td>188844</td>\n",
       "      <td>67526</td>\n",
       "      <td>432447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              city           state  median_age  male_population  \\\n",
       "0         COLUMBIA  SOUTH CAROLINA        28.0            67686   \n",
       "1      BLOOMINGTON       MINNESOTA        40.9            43318   \n",
       "2       UNION CITY      NEW JERSEY        35.4            35376   \n",
       "3          GARLAND           TEXAS        34.5           116406   \n",
       "4       SAN ANGELO           TEXAS        32.8            49669   \n",
       "5   CITRUS HEIGHTS      CALIFORNIA        37.8            41982   \n",
       "6          GILBERT         ARIZONA        33.2           116711   \n",
       "7     APPLE VALLEY      CALIFORNIA        34.3            32873   \n",
       "8        DALY CITY      CALIFORNIA        39.7            53817   \n",
       "9         CARLSBAD      CALIFORNIA        42.1            55119   \n",
       "10          CLOVIS      CALIFORNIA        37.8            52392   \n",
       "11         SPOKANE      WASHINGTON        36.6           102756   \n",
       "12     BAKERSFIELD      CALIFORNIA        30.6           182154   \n",
       "13        KIRKLAND      WASHINGTON        40.8            42159   \n",
       "14        LAKEWOOD        COLORADO        37.7            76013   \n",
       "15        REDLANDS      CALIFORNIA        36.2            33993   \n",
       "16        EL CAJON      CALIFORNIA        32.7            54450   \n",
       "17   SANTA BARBARA      CALIFORNIA        37.8            45068   \n",
       "18        SYRACUSE        NEW YORK        30.3            69462   \n",
       "19       NASHVILLE       TENNESSEE        34.1           314231   \n",
       "\n",
       "    female_population  total_population  total_veterans  foreign_born  \\\n",
       "0               65707            133393            5708          6074   \n",
       "1               43118             86436            6176         10728   \n",
       "2               33773             69149             705         40553   \n",
       "3              120430            236836           10407         62975   \n",
       "4               50744            100413            8412          7859   \n",
       "5               45071             87053            6171          9466   \n",
       "6              130812            247523           10817         24531   \n",
       "7               39312             72185            5714          5801   \n",
       "8               52757            106574            3782         56640   \n",
       "9               58347            113466            6031         17689   \n",
       "10              51780            104172            6173         13409   \n",
       "11             110511            213267           18044         13253   \n",
       "12             191473            373627           12284         71575   \n",
       "13              45108             87267            5156         18570   \n",
       "14              76576            152589            9988         14169   \n",
       "15              37035             71028            3445          8681   \n",
       "16              49238            103688            7103         31865   \n",
       "17              46784             91852            4518         19441   \n",
       "18              74690            144152            5845         17733   \n",
       "19             340365            654596           27942         88193   \n",
       "\n",
       "    average_household_size  indigenous  asian   black  latinx   white  \n",
       "0                     2.32        1420   3501   56398    7545   73232  \n",
       "1                     2.30        1745   4689    5828    8021   71874  \n",
       "2                     2.85         545   4044    4686   53174   50031  \n",
       "3                     3.12        3083  27217   40507   90989  154484  \n",
       "4                     2.50         548   1658    6079   42494   86655  \n",
       "5                     2.49        1514   3615    5979   14661   77525  \n",
       "6                     3.20        5965  19740    9076   39937  211322  \n",
       "7                     3.03        1446   2281    9124   25928   60767  \n",
       "8                     3.26         893  61918    5401   24316   25522  \n",
       "9                     2.68        1513  11948     876   12969   98705  \n",
       "10                    2.76        1876  14249    3434   23744   78029  \n",
       "11                    2.34        7192   9059   10045   13365  193143  \n",
       "12                    3.23        7357  31929   34240  180241  272853  \n",
       "13                    2.30        1155  15277    1041    6720   70491  \n",
       "14                    2.29        2597   7340    2679   33630  139669  \n",
       "15                    2.82         811   6255    6973   19698   55241  \n",
       "16                    3.14        1891   4561    7534   31542   84703  \n",
       "17                    2.51        1560   4964    2013   31102   73659  \n",
       "18                    2.39        3606   9386   46026   13368   88679  \n",
       "19                    2.39        5474  27355  188844   67526  432447  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tidy the ethnicity column names\n",
    "demog_df = demog_df.select(col('city'),\n",
    "                           col('state'),\n",
    "                           col('median_age'),\n",
    "                           col('male_population'),\n",
    "                           col('female_population'),\n",
    "                           col('total_population'),\n",
    "                           col('total_veterans'),\n",
    "                           col('foreign_born'),\n",
    "                           col('average_household_size'),\n",
    "                           col('American Indian and Alaska Native').alias('indigenous'),\n",
    "                           col('Asian').alias('asian'),\n",
    "                           col('Black or African-American').alias('black'),\n",
    "                           col('Hispanic or Latino').alias('latinx'),\n",
    "                           col('White').alias('white'),\n",
    "                          ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data\n",
    "\n",
    "We saw in our data exploration that there were rows with missing AverageTemperature. These rows will be useless for our purposes, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df=temp_df.filter(temp_df.AverageTemperature != 'NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will also see shortly in our data model that the columns `AverageTemperatureUncertainty`, `Latitude` and `Longitude` are not relevant to our inquiry, so we will drop them and take the opportunity to rename our remaining columns, as well as to upprecase the city and country values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.select(col('dt').alias('date'),\n",
    "                             col('AverageTemperature').alias('average_temperature'),\n",
    "                             upper(col('City')).alias('city'),\n",
    "                             upper(col('Country')).alias('country')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>average_temperature</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1772-10-01</td>\n",
       "      <td>10.108</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1775-07-01</td>\n",
       "      <td>18.487</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786-09-01</td>\n",
       "      <td>11.781</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1828-03-01</td>\n",
       "      <td>2.404</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1850-09-01</td>\n",
       "      <td>11.837</td>\n",
       "      <td>ÅRHUS</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  average_temperature   city  country\n",
       "0 1772-10-01               10.108  ÅRHUS  DENMARK\n",
       "1 1775-07-01               18.487  ÅRHUS  DENMARK\n",
       "2 1786-09-01               11.781  ÅRHUS  DENMARK\n",
       "3 1828-03-01                2.404  ÅRHUS  DENMARK\n",
       "4 1850-09-01               11.837  ÅRHUS  DENMARK"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Additionally, our i94 dataset is specific to the year 2016. Our inquiry concerns itself with the factors that may have influenced visitors to choose the destinations they chose. The temperature dataset contains data from as far back as 1743. For our context, we will assume that people would not concern themselves with average temperatures more than 10 years old when choosing where to go in the short term. Hence, we will drop historical data from before 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323360"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop pre-2006 data, and make sure the date field doesn't get a midnight timestamp added (useless for our purposes)\n",
    "temp_df = temp_df.filter(temp_df.date >= '2006-01-01').withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "# temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+-------+\n",
      "|      date|average_temperature|  city|country|\n",
      "+----------+-------------------+------+-------+\n",
      "|2008-10-01|              9.587| Århus|Denmark|\n",
      "|2009-07-01|             23.904|Ürümqi|  China|\n",
      "|2010-01-01|             -13.03|Ürümqi|  China|\n",
      "|2010-07-01|             18.175|Abakan| Russia|\n",
      "|2010-10-01|              2.843|Abakan| Russia|\n",
      "+----------+-------------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
