{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "The purpose of this project is to create an ETL pipeline that wrangles data on immigration, demographics and environmental factors (specifically temperature), in order to be able to glean insights about what factors may make certain US destination popular for international visits. Example questions that business users may want answered include:\n",
    "* Is there a correlation between a visitor's home temperature and the temperature of their chosen destination?\n",
    "* Do people from certain countries prefer destination cities where certain ethnicities are better represented?\n",
    "* Are particular destinations more popular with holders of visas of a certain type?\n",
    "\n",
    "We'll be using Spark to process the data.\n",
    "\n",
    "The project is broken down into 5 steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Clean Up the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's get ready by importing the libraries and tools we'll be using...\n",
    "\n",
    "import pandas as pd\n",
    "import helpers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, upper, first\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ...and initializing our SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scoping the Project and Gathering Data\n",
    "\n",
    "### Scope \n",
    "We have various data on immigration, travel, temperature and demographics, from various sources. The business need is to be able to query the data to see what factors may affect immigration or US travel destination choices. More specifically, the goal is to create a set of relational tables against which we can run queries to look for patterns between chosen immigration or travel destination, and demographic and environmental factors.\n",
    "\n",
    "To accomplish this, Spark will be used, as it is well suited to process large amounts of data across multiple servers. If we wanted to take the project further, we could migrate the output data into a suitable RDBMS, like Redshift, and we could automate the pipleline with Airflow; this, however, is outside the scope of this particular project. The primary goal and scope of the task here is to explore the data, model it and load it into relational tables suitable for analytic querying.\n",
    "\n",
    "\n",
    "### Describing and Gathering the Data \n",
    "The source data are the following:\n",
    "\n",
    "* _I94 Immigration Data_: This data comes from the [US National Tourism and Trade Office](https://www.trade.gov/i-94-arrivals-program). It consists of the 2016 I-94 visitor arrivals data, providing information on arrivals for US visitors who stay 1 night or more. The link to the original dataset is defunct, but a copy of the data was captured and provided by Udacity.\n",
    "* _World Temperature Data_: [This dataset](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) was sourced from Kaggle. It consists of a table of global land temperatures by city and by month, from approximately 1750.\n",
    "* _U.S. City Demographic Data_: [This data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) comes from OpenSoft. The data is compiled from the US Census Bureau's 2015 American Community Survey. It contains demographic information for all US cities that have a population of 65,000 or more. \n",
    "* _Airport Code Table_: [This](https://datahub.io/core/airport-codes#data) is a table of airport codes and corresponding cities.\n",
    "\n",
    "To create the relational tables that will allows us to explore demographic and environmental correlations, we will only need data from the first three sets. We will not be using the _Airport Code Table_ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Exploring and Cleaning Up the Data\n",
    "Let's identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Some of this data was explored \"offline\", so to speak; information was gathered by visiting the original sources for the datasets or via supplemental contextual information. The i94 dataset, particularly, benefited from this; the definitions of the columns was understood primarily through looking at the `I94_SAS_Labels_Descriptions.SAS` file located in the same directory as this README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exploration\n",
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the i94 data\n",
    "i94_df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(i94_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the demographic data\n",
    "demog_df = spark.read.csv('us-cities-demographics.csv', inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(demog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the temperature data\n",
    "temp_df = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column, but first we cast the datetime column to string\n",
    "temp_df_string = temp_df.withColumn(\"dt\",col(\"dt\").cast(StringType()))\n",
    "helpers.show_total_missing_values(temp_df_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "#### Immigration Data\n",
    "If we look again at the table of total missing values per column in the _Exploration_ section, we can see that three columns have a total of more than 85% missing values: `occup`, `entdepu`, and `insnum`. These can be dropped.\n",
    "\n",
    "Taking this a step further, as we'll see shortly in our data model, there are only 11 columns we're interested in keeping. Let's select these to make the dataframe easier to work with and more legible to display. While we're at it, let's rename those columns and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.select(col('cicid').alias('id'),\n",
    "                             col('i94cit').alias('citizenship_country'),\n",
    "                             col('i94res').alias('residence_country'),\n",
    "                             col('i94port').alias('port_of_entry'),\n",
    "                             col('i94addr').alias('destination_state'),\n",
    "                             col('arrdate').alias('arrival_date'),\n",
    "                             col('depdate').alias('departure_date'),\n",
    "                             col('i94bir').alias('age'),\n",
    "                             col('i94visa').alias('visa_type'),\n",
    "                             col('dtaddto').alias('admitted_until'),\n",
    "                             col('gender').alias('gender')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We'll also convert the SAS dates in `arrdate` and `depdate`, and the string date in `dtaddto`, to PySpark dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.withColumn('arrival_date', helpers.convert_sas_date_to_datetime(i94_df.arrival_date)).cache()\n",
    "i94_df = i94_df.withColumn('departure_date', helpers.convert_sas_date_to_datetime(i94_df.departure_date)).cache()\n",
    "i94_df = i94_df.withColumn('admitted_until', to_date(col('admitted_until'),'MMddyyyy')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see that the fields `citizenship_country`, `residence_country`, and `state_of_arrival` are referenced by codes. These need to be cross-referenced with a list of codes that is currently only available in .SAS file. We'll update these values with their full names.\n",
    "\n",
    "We also have the `port_of_entry` field in the i94 table, which we can decompose into port city, port state, and port country. Doing this will later on allow us to add the port code to the temperature and demographic tables, which will allow us to keep our fact table simple, as a single field (`port_of_entry`) will be able to link three dimension tables together. \n",
    "\n",
    "All of this will serve to:\n",
    "1. make the values in our tables more meaningful, and\n",
    "2. make it easier to join tables when querying by reducing the number of joins and tables involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read our i94 fields and values legend \n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    i94_desc = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get country names by code\n",
    "country_codes = {}\n",
    "for countries in i94_desc[10:298]:\n",
    "    pair = countries.split('=')\n",
    "    code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    country_codes[code] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "origin_countries_df = spark.createDataFrame(list(country_codes.items()), ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "| 324|     ANGOLA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "origin_countries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get city names and state abbreviations by numerical codes\n",
    "port_cities = {}\n",
    "port_states = {}\n",
    "broken_fields = {}\n",
    "for cities in i94_desc[303:893]:   \n",
    "    pair = cities.split('=')\n",
    "    code, location = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip()\n",
    "    city_and_state = location.split(',')\n",
    "    if len(city_and_state) == 2:\n",
    "        city, state = city_and_state[0].strip().strip(\"'\"), city_and_state[1].strip().strip(\"'\").strip()\n",
    "        port_cities[code] = city\n",
    "        port_states[code] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df = spark.createDataFrame(list(port_cities.items()), ['code', 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                city|\n",
      "+----+--------------------+\n",
      "| ANC|           ANCHORAGE|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|       DALTONS CACHE|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "| DTH|        DUTCH HARBOR|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_cities_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get state names by state abbreviation\n",
    "state_abbrs = {}\n",
    "for states in i94_desc[982:1035]:   \n",
    "    pair = states.split('=')\n",
    "    abbr, state = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip().strip(\"'\")\n",
    "    if 'N.' in state:\n",
    "        state = state.replace('N.','NORTH')\n",
    "    if 'S.' in state:\n",
    "        state = state.replace('S.','SOUTH')\n",
    "    if 'W.' in state:\n",
    "        state = state.replace('W.','WEST')\n",
    "    if 'DIST.' in state:\n",
    "        state = state.replace('DIST.','DISTRICT')\n",
    "    state_abbrs[abbr] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_countries = {}\n",
    "for code in port_states: \n",
    "    if port_states[code] in state_abbrs:\n",
    "        port_states[code] = state_abbrs[port_states[code]]\n",
    "        port_countries[code] = 'UNITED STATES'\n",
    "    else:\n",
    "        port_countries[code] = port_states[code]\n",
    "        port_states[code] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_abbrs_df = spark.createDataFrame(list(state_abbrs.items()), ['abbr','state'])\n",
    "port_states_df = spark.createDataFrame(list(port_states.items()), ['code','state'])\n",
    "port_countries_df = spark.createDataFrame(list(port_countries.items()), ['code','country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|abbr|     state|\n",
      "+----+----------+\n",
      "|  AK|    ALASKA|\n",
      "|  AZ|   ARIZONA|\n",
      "|  AR|  ARKANSAS|\n",
      "|  CA|CALIFORNIA|\n",
      "|  CO|  COLORADO|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_abbrs_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|code| state|\n",
      "+----+------+\n",
      "| ANC|ALASKA|\n",
      "| BAR|ALASKA|\n",
      "| DAC|ALASKA|\n",
      "| PIZ|ALASKA|\n",
      "| DTH|ALASKA|\n",
      "+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_states_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+\n",
      "|code|      country|\n",
      "+----+-------------+\n",
      "| ANC|UNITED STATES|\n",
      "| BAR|UNITED STATES|\n",
      "| DAC|UNITED STATES|\n",
      "| PIZ|UNITED STATES|\n",
      "| DTH|UNITED STATES|\n",
      "+----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_countries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create views from the dictionary dataframes we just created, \n",
    "# so we can use sql to add meaningful port city, state and country values to our i94 table\n",
    "i94_df.createOrReplaceTempView('i94_view')\n",
    "origin_countries_df.createOrReplaceTempView('origin_countries_view')\n",
    "state_abbrs_df.createOrReplaceTempView('state_abbrs_view')\n",
    "port_cities_df.createOrReplaceTempView('port_cities_view')\n",
    "port_states_df.createOrReplaceTempView('port_states_view')\n",
    "port_countries_df.createOrReplaceTempView('port_countries_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up our i94 table by converting codes to human-readable values\n",
    "i94_clean_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        i94.id AS immigration_id,\n",
    "        i94.age AS age,\n",
    "        i94.gender AS gender,\n",
    "        c_cit.country AS citizenship_country,\n",
    "        c_res.country AS residence_country,\n",
    "        i94.arrival_date AS arrival_date,\n",
    "        i94.departure_date AS departure_date,\n",
    "        i94.visa_type AS visa_type,\n",
    "        i94.admitted_until AS admitted_until,\n",
    "        i94.port_of_entry AS port_of_entry,\n",
    "        p_cit.city AS port_city,\n",
    "        ps.state AS port_state,\n",
    "        p_country.country AS port_country,\n",
    "        sa.state AS destination_state\n",
    "    FROM i94_view i94\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.citizenship_country\n",
    "    LEFT JOIN origin_countries_view c_cit\n",
    "    ON i94.citizenship_country = c_cit.code\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.residence_country\n",
    "    LEFT JOIN origin_countries_view c_res\n",
    "    ON i94.residence_country = c_res.code\n",
    "    \n",
    "    -- retrieve the city names of the code values in i94.port_of_entry\n",
    "    LEFT JOIN port_cities_view p_cit\n",
    "    ON i94.port_of_entry = p_cit.code\n",
    "    \n",
    "    -- retrieve the state names of the code values in i94.port_of_entry, if applicable\n",
    "    LEFT JOIN port_states_view ps\n",
    "    ON i94.port_of_entry = ps.code\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.port_of_entry\n",
    "    LEFT JOIN port_countries_view p_country\n",
    "    ON i94.port_of_entry = p_country.code\n",
    "    \n",
    "    -- retrieve the full state names of the state abbreviations in i94.destination_state\n",
    "    LEFT JOIN state_abbrs_view sa\n",
    "    ON i94.destination_state = sa.abbr\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+-------------+----------------+----------+-------------+-----------------+\n",
      "|       id|arrival_date|departure_date| age|visa_type|admitted_until|gender|citizenship_country|residence_country|port_of_entry|       port_city|port_state| port_country|destination_state|\n",
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+-------------+----------------+----------+-------------+-----------------+\n",
      "|4651287.0|  2016-04-24|    2016-04-29|45.0|      2.0|    2016-10-24|     F|               null|             null|          NEW|NEWARK/TETERBORO|NEW JERSEY|UNITED STATES|             null|\n",
      "|2014736.0|  2016-04-11|    2016-04-15|38.0|      2.0|    2016-07-09|  null|               null|          GERMANY|          HOU|         HOUSTON|     TEXAS|UNITED STATES|             null|\n",
      "|2093085.0|  2016-04-11|    2016-04-15|46.0|      1.0|    2016-10-11|     M|               null|             null|          HOU|         HOUSTON|     TEXAS|UNITED STATES|             null|\n",
      "| 998502.0|  2016-04-06|    2016-04-10|32.0|      2.0|    2016-07-04|     M|              SPAIN|            SPAIN|          HOU|         HOUSTON|     TEXAS|UNITED STATES|             null|\n",
      "|1373083.0|  2016-04-08|    2016-04-09|22.0|      1.0|    2016-10-07|  null|     UNITED KINGDOM|   UNITED KINGDOM|          ATL|         ATLANTA|   GEORGIA|UNITED STATES|             null|\n",
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+-------------+----------------+----------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_clean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|   destination_state| count|\n",
      "+--------------------+------+\n",
      "|          NEW JERSEY| 76531|\n",
      "|        PENNSYLVANIA| 30293|\n",
      "|            ILLINOIS| 82126|\n",
      "|DISTRICT OF COLUMBIA| 28228|\n",
      "|            MARYLAND| 25360|\n",
      "|       WEST VIRGINIA|   808|\n",
      "|               IDAHO|  1752|\n",
      "|            MISSOURI|  8484|\n",
      "|             MONTANA|  1339|\n",
      "|            MICHIGAN| 32101|\n",
      "|             FLORIDA|621701|\n",
      "|                null|187354|\n",
      "|              OREGON| 12574|\n",
      "|        SOUTH DAKOTA|   557|\n",
      "|           LOUISIANA| 22655|\n",
      "|              ALASKA|  1604|\n",
      "|         PUERTO RICO|  9474|\n",
      "|      VIRGIN ISLANDS|   226|\n",
      "|               MAINE|  2361|\n",
      "|       NEW HAMPSHIRE|  2817|\n",
      "|            OKLAHOMA|  3239|\n",
      "|            VIRGINIA| 31399|\n",
      "|          WASHINGTON| 55792|\n",
      "|      NORTH CAROLINA| 23375|\n",
      "|             WYOMING|   460|\n",
      "|               TEXAS|134321|\n",
      "|            NEBRASKA| 26574|\n",
      "|           MINNESOTA| 11194|\n",
      "|              HAWAII|168764|\n",
      "|                GUAM| 94107|\n",
      "|        RHODE ISLAND|  3289|\n",
      "|         MISSISSIPPI|  1771|\n",
      "|           TENNESSEE| 12105|\n",
      "|           WISCONSON|  7860|\n",
      "|            COLORADO| 15874|\n",
      "|              NEVADA|114609|\n",
      "|             VERMONT|  1477|\n",
      "|          NEW MEXICO|  1994|\n",
      "|            NEW YORK|553677|\n",
      "|                UTAH|  7551|\n",
      "|          CALIFORNIA|470386|\n",
      "|                IOWA|  3391|\n",
      "|              KANSAS|  3224|\n",
      "|             ARIZONA| 20218|\n",
      "|            KENTUCKY|  5790|\n",
      "|                OHIO| 18089|\n",
      "|       MASSACHUSETTS| 70486|\n",
      "|      SOUTH CAROLINA|  9811|\n",
      "|            DELAWARE|  3111|\n",
      "|         CONNECTICUT| 13991|\n",
      "|        NORTH DAKOTA|  1225|\n",
      "|            ARKANSAS|  2873|\n",
      "|             INDIANA| 11278|\n",
      "|             GEORGIA| 44663|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of our destination_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('destination_state').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|           port_city| count|\n",
      "+--------------------+------+\n",
      "|              PIEGAN|   140|\n",
      "|         ANDREWS AFB|     4|\n",
      "|           ANZALDUAS|   677|\n",
      "|      FALCON HEIGHTS|     4|\n",
      "|             ORLANDO|149195|\n",
      "|            SAVANNAH|    29|\n",
      "|         TROUT RIVER|    10|\n",
      "|        PEACE BRIDGE|  4035|\n",
      "|        INDIANAPOLIS|   426|\n",
      "|            PROGRESO|    18|\n",
      "|          SAN YSIDRO|  2874|\n",
      "|             ATLANTA| 92579|\n",
      "|            MANASSAS|     2|\n",
      "|     RIO GRANDE CITY|     1|\n",
      "|               MIAMI|343941|\n",
      "|LAREDO COLUMBIA B...|    72|\n",
      "|              HANNAH|     1|\n",
      "|             MEMPHIS|    41|\n",
      "|              NOONAN|     2|\n",
      "|              BLAINE| 11087|\n",
      "|                RENO|     1|\n",
      "|              MOBILE|     3|\n",
      "|             OAKLAND|  3501|\n",
      "|       ST PETERSBURG|   762|\n",
      "|             FORTUNA|     6|\n",
      "|          GREENVILLE|    10|\n",
      "|            PORTLAND|  5108|\n",
      "|         BEEBE PLAIN|     3|\n",
      "|           NIGHTHAWK|     1|\n",
      "|           VANCOUVER| 12706|\n",
      "|              YSLETA|   344|\n",
      "|MOSES LAKE GRANT ...|     3|\n",
      "|        KEAHOLE-KONA|  4042|\n",
      "|           VANCEBORO|     3|\n",
      "|      PORT CANAVERAL|     1|\n",
      "|          FORT MYERS| 17514|\n",
      "|              SAIPAN| 23628|\n",
      "|             DEL RIO|    51|\n",
      "|           ANCHORAGE|    91|\n",
      "|              DENVER| 18260|\n",
      "|       CHRISTIANSTED|   198|\n",
      "|        NORTH CAICOS|  5197|\n",
      "|         COBURN GORE|    37|\n",
      "|             HOULTON|   198|\n",
      "|STEWART - ORANGE ...|     9|\n",
      "|HANSCOM FIELD - B...|    51|\n",
      "|              LAREDO|   355|\n",
      "|               MINOT|     1|\n",
      "|             PHOENIX| 38890|\n",
      "|              BANGOR|   128|\n",
      "|   ST AUGUSTINE ARPT|     2|\n",
      "|           BALTIMORE|  3476|\n",
      "|    NEWARK/TETERBORO|136122|\n",
      "| VETERAN INTL BRIDGE|    79|\n",
      "|                null|101182|\n",
      "|       POINT ROBERTS|   123|\n",
      "|             TORONTO| 20886|\n",
      "|           LUKEVILLE|    89|\n",
      "|     PORT EVERGLADES|  1083|\n",
      "|WILLOW RUN - YPSI...|     1|\n",
      "|             HOUSTON|101481|\n",
      "|           ANACORTES|     1|\n",
      "|               FARGO|     5|\n",
      "|               LIHUE|  2292|\n",
      "|        JACKSONVILLE|    31|\n",
      "|           ROCHESTER|    85|\n",
      "|             MCALLEN|   512|\n",
      "|             DOUGLAS|    64|\n",
      "|              TUCSON|   219|\n",
      "|                ROMA|    12|\n",
      "|              TURNER|     6|\n",
      "|          LONG BEACH|   884|\n",
      "|             SANFORD| 10159|\n",
      "|      PASO DEL NORTE|   232|\n",
      "|       BEECHER FALLS|     7|\n",
      "|          HAKAI PASS|   772|\n",
      "|  BRIDGE OF AMERICAS|   328|\n",
      "|               PHARR|   228|\n",
      "|       EAST RICHFORD|     1|\n",
      "|            HANSBORO|     8|\n",
      "|CANNON INTL - REN...|   255|\n",
      "|           SANTA ANA|  1625|\n",
      "|           MILWAUKEE|   248|\n",
      "|           LANCASTER|    10|\n",
      "|          CHATEAUGAY|     6|\n",
      "|     FORT LAUDERDALE| 95977|\n",
      "|              ROSEAU|    14|\n",
      "|          MORRISTOWN|    20|\n",
      "|          SACRAMENTO|  2201|\n",
      "|            SAN LUIS|    36|\n",
      "|              AUSTIN|  3034|\n",
      "|       GRAND PORTAGE|    51|\n",
      "|         MORSES LINE|    14|\n",
      "|            VICTORIA|   626|\n",
      "|     ST LUCIE COUNTY|    25|\n",
      "|             SHANNON|  3007|\n",
      "|           NASHVILLE|   943|\n",
      "|    HIGHGATE SPRINGS|  1594|\n",
      "|BRADLEY INTERNATI...|    14|\n",
      "|          BELLINGHAM|     4|\n",
      "+--------------------+------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of port_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('port_city').count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data\n",
    "We'll start by renaming the columns and uppercasing the city and state names, to have consistent data formatting across our different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = demog_df.select(upper(col('City')).alias('city'),\n",
    "                             upper(col('State')).alias('state'),\n",
    "                             col('Median Age').alias('median_age'),\n",
    "                             col('Male Population').alias('male_population'),\n",
    "                             col('Female Population').alias('female_population'),\n",
    "                             col('Total Population').alias('total_population'),\n",
    "                             col('Number of Veterans').alias('total_veterans'),\n",
    "                             col('Foreign-born').alias('foreign_born'),\n",
    "                             col('Average Household Size').alias('average_household_size'),\n",
    "                             col('Race').alias('race'),\n",
    "                             col('Count').alias('race_count')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "While we may want denormalized data in our fact and dimension tables, we want to make sure that the denormalizations make sense for our purposes. If we look closely, we can observe that the `Race` column in the demographic data has a small number of unnecessarily repeating values. Barring a few anomalies, each cityappears as five separate rows with the same data for every field, except for the `Race` column, and the `Count`, which refers to the total number of residents who identify with the associated race in a given row. \n",
    "\n",
    "To simplify our table and our querying, we can simply pivot the `Race` column, meaning that we would take its five possible values and make each of them a single column. The values in the `Count` column would become the values for the associated `Race` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|       city|         state|median_age|male_population|female_population|total_population|total_veterans|foreign_born|average_household_size|American Indian and Alaska Native|Asian|Black or African-American|Hispanic or Latino| White|\n",
      "+-----------+--------------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|   COLUMBIA|SOUTH CAROLINA|      28.0|          67686|            65707|          133393|          5708|        6074|                  2.32|                             1420| 3501|                    56398|              7545| 73232|\n",
      "|BLOOMINGTON|     MINNESOTA|      40.9|          43318|            43118|           86436|          6176|       10728|                   2.3|                             1745| 4689|                     5828|              8021| 71874|\n",
      "| UNION CITY|    NEW JERSEY|      35.4|          35376|            33773|           69149|           705|       40553|                  2.85|                              545| 4044|                     4686|             53174| 50031|\n",
      "|    GARLAND|         TEXAS|      34.5|         116406|           120430|          236836|         10407|       62975|                  3.12|                             3083|27217|                    40507|             90989|154484|\n",
      "| SAN ANGELO|         TEXAS|      32.8|          49669|            50744|          100413|          8412|        7859|                   2.5|                              548| 1658|                     6079|             42494| 86655|\n",
      "+-----------+--------------+----------+---------------+-----------------+----------------+--------------+------------+----------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Turn the Race column into five distinct race columns\n",
    "demog_df = demog_df.groupBy(\"city\",\n",
    "                            \"state\",\n",
    "                            \"median_age\",\n",
    "                            \"male_population\",\n",
    "                            \"female_population\",\n",
    "                            \"total_population\",\n",
    "                            \"total_veterans\", \n",
    "                            \"foreign_born\", \n",
    "                            \"average_household_size\"\n",
    "                           ).pivot(\"race\").agg(first(\"race_count\"))\n",
    "demog_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Tidy the ethnicity column names\n",
    "demog_df = demog_df.select(col('city'),\n",
    "                           col('state'),\n",
    "                           col('median_age'),\n",
    "                           col('male_population'),\n",
    "                           col('female_population'),\n",
    "                           col('total_population'),\n",
    "                           col('total_veterans'),\n",
    "                           col('foreign_born'),\n",
    "                           col('average_household_size').alias('avg_household_size'),\n",
    "                           col('American Indian and Alaska Native').alias('indigenous'),\n",
    "                           col('Asian').alias('asian'),\n",
    "                           col('Black or African-American').alias('black'),\n",
    "                           col('Hispanic or Latino').alias('latinx'),\n",
    "                           col('White').alias('white'),\n",
    "                          ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demog_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_population</th>\n",
       "      <th>female_population</th>\n",
       "      <th>total_population</th>\n",
       "      <th>total_veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>average_household_size</th>\n",
       "      <th>indigenous</th>\n",
       "      <th>asian</th>\n",
       "      <th>black</th>\n",
       "      <th>latinx</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COLUMBIA</td>\n",
       "      <td>SOUTH CAROLINA</td>\n",
       "      <td>28.0</td>\n",
       "      <td>67686</td>\n",
       "      <td>65707</td>\n",
       "      <td>133393</td>\n",
       "      <td>5708</td>\n",
       "      <td>6074</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1420</td>\n",
       "      <td>3501</td>\n",
       "      <td>56398</td>\n",
       "      <td>7545</td>\n",
       "      <td>73232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLOOMINGTON</td>\n",
       "      <td>MINNESOTA</td>\n",
       "      <td>40.9</td>\n",
       "      <td>43318</td>\n",
       "      <td>43118</td>\n",
       "      <td>86436</td>\n",
       "      <td>6176</td>\n",
       "      <td>10728</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1745</td>\n",
       "      <td>4689</td>\n",
       "      <td>5828</td>\n",
       "      <td>8021</td>\n",
       "      <td>71874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNION CITY</td>\n",
       "      <td>NEW JERSEY</td>\n",
       "      <td>35.4</td>\n",
       "      <td>35376</td>\n",
       "      <td>33773</td>\n",
       "      <td>69149</td>\n",
       "      <td>705</td>\n",
       "      <td>40553</td>\n",
       "      <td>2.85</td>\n",
       "      <td>545</td>\n",
       "      <td>4044</td>\n",
       "      <td>4686</td>\n",
       "      <td>53174</td>\n",
       "      <td>50031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GARLAND</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>34.5</td>\n",
       "      <td>116406</td>\n",
       "      <td>120430</td>\n",
       "      <td>236836</td>\n",
       "      <td>10407</td>\n",
       "      <td>62975</td>\n",
       "      <td>3.12</td>\n",
       "      <td>3083</td>\n",
       "      <td>27217</td>\n",
       "      <td>40507</td>\n",
       "      <td>90989</td>\n",
       "      <td>154484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SAN ANGELO</td>\n",
       "      <td>TEXAS</td>\n",
       "      <td>32.8</td>\n",
       "      <td>49669</td>\n",
       "      <td>50744</td>\n",
       "      <td>100413</td>\n",
       "      <td>8412</td>\n",
       "      <td>7859</td>\n",
       "      <td>2.50</td>\n",
       "      <td>548</td>\n",
       "      <td>1658</td>\n",
       "      <td>6079</td>\n",
       "      <td>42494</td>\n",
       "      <td>86655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          city           state  median_age  male_population  \\\n",
       "0     COLUMBIA  SOUTH CAROLINA        28.0            67686   \n",
       "1  BLOOMINGTON       MINNESOTA        40.9            43318   \n",
       "2   UNION CITY      NEW JERSEY        35.4            35376   \n",
       "3      GARLAND           TEXAS        34.5           116406   \n",
       "4   SAN ANGELO           TEXAS        32.8            49669   \n",
       "\n",
       "   female_population  total_population  total_veterans  foreign_born  \\\n",
       "0              65707            133393            5708          6074   \n",
       "1              43118             86436            6176         10728   \n",
       "2              33773             69149             705         40553   \n",
       "3             120430            236836           10407         62975   \n",
       "4              50744            100413            8412          7859   \n",
       "\n",
       "   average_household_size  indigenous  asian  black  latinx   white  \n",
       "0                    2.32        1420   3501  56398    7545   73232  \n",
       "1                    2.30        1745   4689   5828    8021   71874  \n",
       "2                    2.85         545   4044   4686   53174   50031  \n",
       "3                    3.12        3083  27217  40507   90989  154484  \n",
       "4                    2.50         548   1658   6079   42494   86655  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data\n",
    "\n",
    "We saw in our data exploration that there were rows with missing AverageTemperature. These rows will be useless for our purposes, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df=temp_df.filter(temp_df.AverageTemperature != 'NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will also see shortly in our data model that the columns `AverageTemperatureUncertainty`, `Latitude` and `Longitude` are not relevant to our inquiry, so we will drop them and take the opportunity to rename our remaining columns, as well as to upprecase the city and country values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.select(col('dt').alias('date'),\n",
    "                             col('AverageTemperature').alias('avg_temperature'),\n",
    "                             upper(col('City')).alias('city'),\n",
    "                             upper(col('Country')).alias('country')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>average_temperature</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>11.605</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>19.032</td>\n",
       "      <td>CHESAPEAKE</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>AALBORG</td>\n",
       "      <td>DENMARK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-09-01</td>\n",
       "      <td>13.380</td>\n",
       "      <td>CHERNIVTSI</td>\n",
       "      <td>UKRAINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-09-01</td>\n",
       "      <td>15.424</td>\n",
       "      <td>CLEVELAND</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  average_temperature        city        country\n",
       "0 1744-05-01               11.605     CHICAGO  UNITED STATES\n",
       "1 1744-05-01               19.032  CHESAPEAKE  UNITED STATES\n",
       "2 1744-06-01               14.051     AALBORG        DENMARK\n",
       "3 1744-09-01               13.380  CHERNIVTSI        UKRAINE\n",
       "4 1744-09-01               15.424   CLEVELAND  UNITED STATES"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Additionally, our i94 dataset is specific to the year 2016. Our inquiry concerns itself with the factors that may have influenced visitors to choose the destinations they chose. The temperature dataset contains data from as far back as 1743. For our context, we will assume that people would not concern themselves with average temperatures more than 10 years old when choosing where to go in the short term. Hence, we will drop historical data from before 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Drop pre-2006 data, and make sure the date field doesn't get a midnight timestamp added (useless for our purposes)\n",
    "temp_df = temp_df.filter(temp_df.date >= '2006-01-01').withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "# temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+----------+-------------+\n",
      "|      date|average_temperature|      city|      country|\n",
      "+----------+-------------------+----------+-------------+\n",
      "|2006-01-01|             -9.868| ASTRAKHAN|       RUSSIA|\n",
      "|2006-01-01|             14.363|COSTA MESA|UNITED STATES|\n",
      "|2006-01-01|             20.465| BALESHWAR|        INDIA|\n",
      "|2006-01-01|             26.955|   BACOLOD|  PHILIPPINES|\n",
      "|2006-02-01|            -15.714|   BAOSHAN|        CHINA|\n",
      "+----------+-------------------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "Data will be transferred into a star schema (one fact table where each dimension tables is referenced by a row; additional, derived information can also be added to the fact table). This schema design facilitates analytical querying, as it denormalizes data according to the particular aspects of the data we want to dig further into. In other words, it's optimized to minimize the number of JOINs required to query the data. \n",
    "\n",
    "#### Fact Table\n",
    "\n",
    "**visits_fact** - immigration events\n",
    "- visit_id, immigration_id, port_code, arrival_date, length_of_stay\n",
    "\n",
    "#### Dimension Tables\n",
    "\n",
    "**immigrations_dim** - users in the app\n",
    "- immigration_id, age, gender, citizenship_country, residence_country, arrival_date, departure_date, visa_type, admitted_until, port_of_entry, port_city, port_state, port_country, destination_state\n",
    "\n",
    "**demographics_dim** - songs in music database\n",
    "- city_id, port_code, city, state, median_age, male_population, female_population, total_population, total_veterans, foreign_born, avg_household_size, indigenous, asian, black, latinx, white\n",
    "\n",
    "**temperature_dim** - artists in music database\n",
    "- temperature_id, port_code, date, avg_temperature, city, country\n",
    "\n",
    "**date_dim** - dates of arrivals, departures, and permitted stays \n",
    "- date, day, week, month, year, weekday\n",
    "\n",
    "![data model](./pipeline_star_schema.jpg \"data model\")\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "The steps necessary to pipeline the data into the above data model will be as follows:\n",
    "1. Clean the i94, temperature and demographics datasets (already performed)\n",
    "2. Create a date_dim dimension table based on the date values in the other tables, and extract the desired fields\n",
    "3. Create temperature_dim dimension table: \n",
    "    1. Load the existing, cleaned up temperature table, adding a unique id column\n",
    "    2. Add a port_code field based on the city value (correlated with country name, to disambiguate multiple cities with the same name)\n",
    "4. Create a demographic_dim dimension table:\n",
    "    1. Load the existing, cleaned up demographics table, adding a unique id column\n",
    "    2. Add a port_code field based on the city value (correlated with state and country name, to disambiguate multiple cities with the same name)\n",
    "5. Create the visits_fact table with a derived length_of_stay column, mapping immigration_id from immigrations_dim, a port_code that can be referenced in temperature_dim and demographics_dim, and the arrival_date associated with the immigration_id\n",
    "6. Write these tables to parquet files that can later be queried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
