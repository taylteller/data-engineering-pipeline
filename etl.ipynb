{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "## Data Engineering Capstone Project\n",
    "\n",
    "### Project Summary\n",
    "The purpose of this project is to create an ETL pipeline that wrangles data on immigration, demographics and environmental factors (specifically temperature), in order to be able to glean insights about what factors may make certain US destination popular for international visits. Example questions that business users may want answered include:\n",
    "* Is there a correlation between a visitor's home temperature and the temperature of their chosen destination?\n",
    "* Do people from certain countries prefer destination cities where certain ethnicities are better represented?\n",
    "* Are particular destinations more popular with holders of visas of a certain type?\n",
    "\n",
    "We'll be using Spark to process the data.\n",
    "\n",
    "The project is broken down into 5 steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Clean Up the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Let's get ready by importing the libraries and tools we'll be using...\n",
    "\n",
    "import pandas as pd\n",
    "import helpers\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# ...and initializing our SparkSession\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scoping the Project and Gathering Data\n",
    "\n",
    "### Scope \n",
    "We have various data on immigration, travel, temperature and demographics, from various sources. The business need is to be able to query the data to see what factors may affect immigration or US travel destination choices. More specifically, the goal is to create a set of relational tables against which we can run queries to look for patterns between chosen immigration or travel destination, and demographic and environmental factors.\n",
    "\n",
    "To accomplish this, Spark will be used, as it is well suited to process large amounts of data across multiple servers. If we wanted to take the project further, we could migrate the output data into a suitable RDBMS, like Redshift, and we could automate the pipleline with Airflow; this, however, is outside the scope of this particular project. The primary goal and scope of the task here is to explore the data, model it and load it into relational tables suitable for analytic querying.\n",
    "\n",
    "\n",
    "### Describing and Gathering the Data \n",
    "The source data are the following:\n",
    "\n",
    "* _I94 Immigration Data_: This data comes from the [US National Tourism and Trade Office](https://www.trade.gov/i-94-arrivals-program). It consists of the 2016 I-94 visitor arrivals data, providing information on arrivals for US visitors who stay 1 night or more. The link to the original dataset is defunct, but a copy of the data was captured and provided by Udacity.\n",
    "* _World Temperature Data_: [This dataset](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) was sourced from Kaggle. It consists of a table of global land temperatures by city and by month, from approximately 1750.\n",
    "* _U.S. City Demographic Data_: [This data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/) comes from OpenSoft. The data is compiled from the US Census Bureau's 2015 American Community Survey. It contains demographic information for all US cities that have a population of 65,000 or more. \n",
    "* _Airport Code Table_: [This](https://datahub.io/core/airport-codes#data) is a table of airport codes and corresponding cities.\n",
    "\n",
    "To create the relational tables that will allows us to explore demographic and environmental correlations, we will only need data from the first three sets. We will not be using the _Airport Code Table_ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Exploring and Cleaning Up the Data\n",
    "Let's identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "Some of this data was explored \"offline\", so to speak; information was gathered by visiting the original sources for the datasets or via supplemental contextual information. The i94 dataset, particularly, benefited from this; the definitions of the columns was understood primarily through looking at the `I94_SAS_Labels_Descriptions.SAS` file located in the same directory as this README file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Exploration\n",
    "#### Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the i94 data\n",
    "i94_df = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(i94_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the demographic data\n",
    "demog_df = spark.read.csv('us-cities-demographics.csv', inferSchema=True, header=True, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Get a preview\n",
    "demog_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the total of null or NaN values per column\n",
    "helpers.show_total_missing_values(demog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the temperature data\n",
    "temp_df = spark.read.csv('../../data2/GlobalLandTemperaturesByCity.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0 1743-11-01               6.068                          1.737  Århus   \n",
       "1 1743-12-01                 NaN                            NaN  Århus   \n",
       "2 1744-01-01                 NaN                            NaN  Århus   \n",
       "3 1744-02-01                 NaN                            NaN  Århus   \n",
       "4 1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a preview\n",
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column name</th>\n",
       "      <th>total missing</th>\n",
       "      <th>% missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dt</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AverageTemperature</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AverageTemperatureUncertainty</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     column name  total missing  % missing\n",
       "0                             dt              0        0.0\n",
       "1             AverageTemperature              0        0.0\n",
       "2  AverageTemperatureUncertainty              0        0.0\n",
       "3                           City              0        0.0\n",
       "4                        Country              0        0.0\n",
       "5                       Latitude              0        0.0\n",
       "6                      Longitude              0        0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the total of null or NaN values per column, but first we cast the datetime column to string\n",
    "temp_df_string = temp_df.withColumn(\"dt\",col(\"dt\").cast(StringType()))\n",
    "helpers.show_total_missing_values(temp_df_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "#### Immigration Data\n",
    "If we look again at the table of total missing values per column in the _Exploration_ section, we can see that three columns have a total of more than 85% missing values: `occup`, `entdepu`, and `insnum`. These can be dropped.\n",
    "\n",
    "Taking this a step further, as we'll see shortly in our data model, there are only 11 columns we're interested in keeping. Let's select these to make the dataframe easier to work with and more legible to display. While we're at it, let's rename those columns and drop duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.select(col('cicid').alias('id'),\n",
    "                             col('i94cit').alias('citizenship_country'),\n",
    "                             col('i94res').alias('residence_country'),\n",
    "                             col('i94port').alias('port_of_entry'),\n",
    "                             col('i94addr').alias('destination_state'),\n",
    "                             col('arrdate').alias('arrival_date'),\n",
    "                             col('depdate').alias('departure_date'),\n",
    "                             col('i94bir').alias('age'),\n",
    "                             col('i94visa').alias('visa_type'),\n",
    "                             col('dtaddto').alias('admitted_until'),\n",
    "                             col('gender').alias('gender')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We'll also convert the SAS dates in `arrdate` and `depdate`, and the string date in `dtaddto`, to PySpark dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.withColumn('arrival_date', helpers.convert_sas_date_to_datetime(i94_df.arrival_date)).cache()\n",
    "i94_df = i94_df.withColumn('departure_date', helpers.convert_sas_date_to_datetime(i94_df.departure_date)).cache()\n",
    "i94_df = i94_df.withColumn('admitted_until', to_date(col('admitted_until'),'MMddyyyy')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can see that the fields `citizenship_country`, `residence_country`, `port_of_entry`, and `state_of_arrival` are referenced by codes. These need to be cross-referenced with a list of codes that is currently only available in text form. Since temperature and demographic data have city, state and country fields written out in full, we can parse the text file of codes and update the values in the i94 dataframe to also be written out in full. This will make it easier to join tables when querying by reducing the number of joins and tables involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read our i94 fields and values legend \n",
    "with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    i94_desc = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get country names by code\n",
    "country_codes = {}\n",
    "for countries in i94_desc[10:298]:\n",
    "    pair = countries.split('=')\n",
    "    code, country = pair[0].strip(), pair[1].strip().strip(\"'\")\n",
    "    country_codes[code] = country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries_df = spark.createDataFrame(list(country_codes.items()), ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "| 324|     ANGOLA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get city names and state abbreviations by numerical codes\n",
    "port_cities = {}\n",
    "port_states = {}\n",
    "broken_fields = {}\n",
    "for cities in i94_desc[303:893]:   \n",
    "    pair = cities.split('=')\n",
    "    code, location = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip()\n",
    "    city_and_state = location.split(',')\n",
    "    if len(city_and_state) == 2:\n",
    "        city, state = city_and_state[0].strip().strip(\"'\"), city_and_state[1].strip().strip(\"'\").strip()\n",
    "        port_cities[code] = city\n",
    "        port_states[code] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df = spark.createDataFrame(list(port_cities.items()), ['code', 'city'])\n",
    "port_states_df = spark.createDataFrame(list(port_states.items()), ['code','state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_cities_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "port_states_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get state names by state abbreviation\n",
    "state_codes = {}\n",
    "for states in i94_desc[982:1035]:   \n",
    "    pair = states.split('=')\n",
    "    code, state = pair[0].strip(\"\\t\").strip().strip(\"'\"), pair[1].strip('\\t').strip().strip(\"'\")\n",
    "    if 'N.' in state:\n",
    "        state = state.replace('N.','NORTH')\n",
    "    if 'S.' in state:\n",
    "        state = state.replace('S.','SOUTH')\n",
    "    if 'W.' in state:\n",
    "        state = state.replace('W.','WEST')\n",
    "    if 'DIST.' in state:\n",
    "        state = state.replace('DIST.','DISTRICT')\n",
    "    state_codes[code] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_codes_df = spark.createDataFrame(list(state_codes.items()), ['code', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+----------------+----------+-----------------+\n",
      "|       id|arrival_date|departure_date| age|visa_type|admitted_until|gender|citizenship_country|residence_country|       port_city|port_state|destination_state|\n",
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+----------------+----------+-----------------+\n",
      "|4651287.0|  2016-04-24|    2016-04-29|45.0|      2.0|    2016-10-24|     F|               null|             null|NEWARK/TETERBORO|NEW JERSEY|             null|\n",
      "|2014736.0|  2016-04-11|    2016-04-15|38.0|      2.0|    2016-07-09|  null|               null|          GERMANY|         HOUSTON|     TEXAS|             null|\n",
      "|2093085.0|  2016-04-11|    2016-04-15|46.0|      1.0|    2016-10-11|     M|               null|             null|         HOUSTON|     TEXAS|             null|\n",
      "| 998502.0|  2016-04-06|    2016-04-10|32.0|      2.0|    2016-07-04|     M|              SPAIN|            SPAIN|         HOUSTON|     TEXAS|             null|\n",
      "|1594614.0|  2016-04-09|    2016-04-10|21.0|      1.0|    2016-10-08|  null|     UNITED KINGDOM|   UNITED KINGDOM|        NEW YORK|  NEW YORK|             null|\n",
      "+---------+------------+--------------+----+---------+--------------+------+-------------------+-----------------+----------------+----------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create views from the dictionary dataframes we just created, \n",
    "# so we can use sql to replace the codes in our i94 table with their more human-usable values\n",
    "i94_df.createOrReplaceTempView('i94_view')\n",
    "countries_df.createOrReplaceTempView('countries_view')\n",
    "port_cities_df.createOrReplaceTempView('port_cities_view')\n",
    "port_states_df.createOrReplaceTempView('port_states_view')\n",
    "state_codes_df.createOrReplaceTempView('state_codes_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# clean up our i94 table by converting codes to human-readable values\n",
    "i94_clean_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        i94.id,\n",
    "        i94.arrival_date AS arrival_date,\n",
    "        i94.departure_date AS departure_date,\n",
    "        i94.age AS age,\n",
    "        i94.visa_type AS visa_type,\n",
    "        i94.admitted_until AS admitted_until,\n",
    "        i94.gender AS gender,\n",
    "        c_cit.country AS citizenship_country,\n",
    "        c_res.country AS residence_country,\n",
    "        pc.city AS port_city,\n",
    "        sc_port.state AS port_state,\n",
    "        sc.state AS destination_state\n",
    "    FROM i94_view i94\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.citizenship_country\n",
    "    LEFT JOIN countries_view c_cit\n",
    "    ON i94.citizenship_country = c_cit.code\n",
    "    \n",
    "    -- retrieve the country names of the code values in i94.residence_country\n",
    "    LEFT JOIN countries_view c_res\n",
    "    ON i94.residence_country = c_res.code\n",
    "    \n",
    "    -- retrieve the city names of the code values in i94.port_of_entry\n",
    "    LEFT JOIN port_cities_view pc\n",
    "    ON i94.port_of_entry = pc.code\n",
    "    \n",
    "    -- retrieve the state abbreviations of the numerical codes in i94.port_of_entry...\n",
    "    -- ...then the full state names of the state abbreviations\n",
    "    LEFT JOIN port_states_view ps\n",
    "    ON i94.port_of_entry = ps.code\n",
    "    LEFT JOIN state_codes_view sc_port\n",
    "    ON ps.state = sc_port.code \n",
    "    \n",
    "    -- retrieve the full state names of the state abbreviations in i94.destination_state\n",
    "    LEFT JOIN state_codes_view sc\n",
    "    ON i94.destination_state = sc.code\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_clean_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|   destination_state| count|\n",
      "+--------------------+------+\n",
      "|          NEW JERSEY| 76531|\n",
      "|        PENNSYLVANIA| 30293|\n",
      "|            ILLINOIS| 82126|\n",
      "|DISTRICT OF COLUMBIA| 28228|\n",
      "|            MARYLAND| 25360|\n",
      "|       WEST VIRGINIA|   808|\n",
      "|               IDAHO|  1752|\n",
      "|            MISSOURI|  8484|\n",
      "|             MONTANA|  1339|\n",
      "|            MICHIGAN| 32101|\n",
      "|             FLORIDA|621701|\n",
      "|                null|187354|\n",
      "|              OREGON| 12574|\n",
      "|        SOUTH DAKOTA|   557|\n",
      "|           LOUISIANA| 22655|\n",
      "|              ALASKA|  1604|\n",
      "|         PUERTO RICO|  9474|\n",
      "|      VIRGIN ISLANDS|   226|\n",
      "|               MAINE|  2361|\n",
      "|       NEW HAMPSHIRE|  2817|\n",
      "|            OKLAHOMA|  3239|\n",
      "|            VIRGINIA| 31399|\n",
      "|          WASHINGTON| 55792|\n",
      "|      NORTH CAROLINA| 23375|\n",
      "|             WYOMING|   460|\n",
      "|               TEXAS|134321|\n",
      "|            NEBRASKA| 26574|\n",
      "|           MINNESOTA| 11194|\n",
      "|              HAWAII|168764|\n",
      "|                GUAM| 94107|\n",
      "|        RHODE ISLAND|  3289|\n",
      "|         MISSISSIPPI|  1771|\n",
      "|           TENNESSEE| 12105|\n",
      "|           WISCONSON|  7860|\n",
      "|            COLORADO| 15874|\n",
      "|              NEVADA|114609|\n",
      "|             VERMONT|  1477|\n",
      "|          NEW MEXICO|  1994|\n",
      "|            NEW YORK|553677|\n",
      "|                UTAH|  7551|\n",
      "|          CALIFORNIA|470386|\n",
      "|                IOWA|  3391|\n",
      "|              KANSAS|  3224|\n",
      "|             ARIZONA| 20218|\n",
      "|            KENTUCKY|  5790|\n",
      "|                OHIO| 18089|\n",
      "|       MASSACHUSETTS| 70486|\n",
      "|      SOUTH CAROLINA|  9811|\n",
      "|            DELAWARE|  3111|\n",
      "|         CONNECTICUT| 13991|\n",
      "|        NORTH DAKOTA|  1225|\n",
      "|            ARKANSAS|  2873|\n",
      "|             INDIANA| 11278|\n",
      "|             GEORGIA| 44663|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of our destination_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('destination_state').count().show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check the distribution of port_state values. Make sure we didn't accidentally end up with all nulls (for example)\n",
    "i94_clean_df.groupBy('port_state').count().show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographic Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature\n",
    "\n",
    "We saw in our data exploration that there were rows with missing AverageTemperature. These rows will be useless for our purposes, so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df=temp_df.filter(temp_df.AverageTemperature != 'NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will also see shortly in our data model that the columns `AverageTemperatureUncertainty`, `Latitude` and `Longitude` are not relevant to our inquiry, so we will drop them and take the opportunity to rename our remaining columns what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df.select(col('dt').alias('date'),\n",
    "                             col('AverageTemperature').alias('average_temperature'),\n",
    "                             col('City').alias('city'),\n",
    "                             col('Country').alias('country')\n",
    "                            ).dropDuplicates().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>average_temperature</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1767-05-01</td>\n",
       "      <td>9.561</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1784-03-01</td>\n",
       "      <td>-2.915</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786-02-01</td>\n",
       "      <td>-1.113</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1794-12-01</td>\n",
       "      <td>0.082</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1830-02-01</td>\n",
       "      <td>-3.842</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  average_temperature   city  country\n",
       "0 1767-05-01                9.561  Århus  Denmark\n",
       "1 1784-03-01               -2.915  Århus  Denmark\n",
       "2 1786-02-01               -1.113  Århus  Denmark\n",
       "3 1794-12-01                0.082  Århus  Denmark\n",
       "4 1830-02-01               -3.842  Århus  Denmark"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Additionally, our i94 dataset is specific to the year 2016. Our inquiry concerns itself with the factors that may have influenced visitors to choose the destinations they chose. The temperature dataset contains data from as far back as 1743. For our context, we will assume that people would not concern themselves with average temperatures more than 10 years old when choosing where to go in the short term. Hence, we will drop historical data from before 2006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323360"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop pre-2006 data, and make sure the date field doesn't get a midnight timestamp added (useless for our purposes)\n",
    "temp_df = temp_df.filter(temp_df.date >= '2006-01-01').withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "# temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+------+-------+\n",
      "|      date|average_temperature|  city|country|\n",
      "+----------+-------------------+------+-------+\n",
      "|2008-10-01|              9.587| Århus|Denmark|\n",
      "|2009-07-01|             23.904|Ürümqi|  China|\n",
      "|2010-01-01|             -13.03|Ürümqi|  China|\n",
      "|2010-07-01|             18.175|Abakan| Russia|\n",
      "|2010-10-01|              2.843|Abakan| Russia|\n",
      "+----------+-------------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Run Pipelines to Model the Data \n",
    "### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
